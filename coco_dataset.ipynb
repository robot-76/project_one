{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision import transforms\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import ToTensor, Resize, Compose, Normalize, RandomHorizontalFlip, RandomVerticalFlip, RandomRotation, RandomResizedCrop, ColorJitter\n",
    "import os, argparse, time, subprocess, io, shlex, pickle, pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import util.misc as utils\n",
    "import tqdm\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3060 Ti'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    raise SystemError('GPU device not found')\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=15.49s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.38s)\n",
      "creating index...\n",
      "index created!\n",
      "dataloader_train 925\n",
      "dataloader_val 40\n",
      "dataloader_shape torch.Size([3, 265, 265])\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "class CocoDatasetLoader:\n",
    "    def __init__(self, coco_root_train, coco_root_val, transform=None, batch_size=128, num_workers=0):\n",
    "        self.transform = transform\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.coco_root_train = os.path.join(coco_root_train)\n",
    "        self.coco_root_val = os.path.join(coco_root_val)\n",
    "\n",
    "    def create_coco_dataset(self, root, ann_file):\n",
    "        return CocoDetection(root=root, annFile=ann_file, transform=self.transform)\n",
    "    \n",
    "    @staticmethod\n",
    "    def target_to_tensor(target):\n",
    "        # Extract bounding boxes and labels\n",
    "        boxes = [obj['bbox'] for obj in target]\n",
    "        labels = [obj['category_id'] for obj in target]\n",
    "        image_ids = [obj['image_id'] for obj in target]\n",
    "        ids = [obj['id'] for obj in target]\n",
    "\n",
    "        # Pad boxes and labels to a fixed length\n",
    "        max_len = 100  # Replace with actual maximum length\n",
    "        while len(boxes) < max_len:\n",
    "            boxes.append([0, 0, 0, 0])\n",
    "            labels.append(0)\n",
    "            image_ids.append(0)\n",
    "            ids.append(0)\n",
    "\n",
    "        # Convert to tensors\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "        image_ids = torch.tensor(image_ids, dtype=torch.int64)\n",
    "        ids = torch.tensor(ids, dtype=torch.int64)\n",
    "\n",
    "        # Concatenate boxes and labels\n",
    "        data = torch.cat((boxes, labels.unsqueeze(1), image_ids.unsqueeze(1), ids.unsqueeze(1)), dim=-1)\n",
    "\n",
    "        return data\n",
    "     \n",
    "    @staticmethod           \n",
    "    def collate_fn(batch):\n",
    "        images = [item[0] for item in batch]\n",
    "        targets = [item[1] for item in batch]\n",
    "\n",
    "        # Ensure images are tensors\n",
    "        images = [torch.from_numpy(img) if not isinstance(img, torch.Tensor) else img for img in images]\n",
    "\n",
    "        # Stack images together\n",
    "        images = torch.stack(images, dim=0)\n",
    "\n",
    "        # Convert targets to tensors\n",
    "        targets = [CocoDatasetLoader.target_to_tensor(t) for t in targets]\n",
    "\n",
    "        return images, targets\n",
    "    \n",
    "    \n",
    "    def get_data_loaders(self):\n",
    "        \n",
    "        coco_dataset_train = self.create_coco_dataset(root=self.coco_root_train, ann_file=\"./annotations/instances_train2017.json\")\n",
    "        coco_dataset_val = self.create_coco_dataset(root=self.coco_root_val, ann_file=\"./annotations/instances_val2017.json\")\n",
    "\n",
    "        dataloader_train = DataLoader(coco_dataset_train, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers, collate_fn=self.collate_fn)\n",
    "        dataloader_val = DataLoader(coco_dataset_val, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers, collate_fn=self.collate_fn)\n",
    "        \n",
    "        return dataloader_train, dataloader_val\n",
    "\n",
    "coco_loader =CocoDatasetLoader(coco_root_train=\"./coco_dataset_test/train2017\", #print the real image and there normalize image and the\n",
    "                                coco_root_val=\"./coco_dataset_test/val2017\",\n",
    "                           \n",
    "                                transform=transforms.Compose([\n",
    "                                    RandomHorizontalFlip(),\n",
    "                                    transforms.Compose([\n",
    "                                    Resize([256,256]),\n",
    "                                    RandomResizedCrop(265, scale=(0.2, 1.0)),\n",
    "                                    ]),\n",
    "                                    ToTensor(),\n",
    "                                    Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                std=[0.229, 0.224, 0.225])\n",
    "                                    \n",
    "                                ])\n",
    ")\n",
    "\n",
    "\n",
    "dataloader_train, dataloader_val = coco_loader.get_data_loaders()\n",
    "\n",
    "\n",
    "print(f\"dataloader_train\",len(dataloader_train))\n",
    "print(f\"dataloader_val\",len(dataloader_val))\n",
    "print(f\"dataloader_shape\",dataloader_train.dataset[0][0].shape)\n",
    "#print(f\"dataloader_shape\",dataloader_train.dataset[0][1].shape)\n",
    "\n",
    "for batch in dataloader_train:\n",
    "    images, targets = batch\n",
    "    targets =[ target.to(device) for target in targets]\n",
    "    images = list(img.to(device) for img in images)\n",
    "    #print(\"targets\",targets)\n",
    "     \n",
    "  \n",
    "    #print(F\"targets=\", targets)\n",
    "    #print(F\"images=\", images)\n",
    "    \n",
    "    #for i, image in enumerate(images):\n",
    "        #print(f\"Shape of image {i}: {image.shape}\")\n",
    "        \n",
    "    #images_tensor = torch.stack(images)\n",
    "    #print(images_tensor.shape)\n",
    "    \n",
    "     # Get color images (assuming images are in RGB format)\n",
    "    #color_images = [F.to_pil_image(image.cpu()) for image in images]\n",
    "\n",
    "    # Display color images\n",
    "    #for i, color_image in enumerate(color_images):\n",
    "        #plt.imshow(color_image)\n",
    "        #plt.show()\n",
    "\n",
    "    # Access tensors (boxes, labels, image_ids, ids) for further processing\n",
    "    for target in targets:\n",
    "        #Access target data, e.g., boxes, labels, etc.\n",
    "        boxes = target[:, :4]\n",
    "        labels = target[:, 4]\n",
    "        image_ids = target[:, 5]\n",
    "        ids = target[:, 6]\n",
    "        #print(boxes, labels, image_ids, ids)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "       \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backbone Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x torch.Size([128, 557568])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 75\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[0;32m     74\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 75\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     78\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100.0\u001b[39m \u001b[38;5;241m*\u001b[39m correct_samples \u001b[38;5;241m/\u001b[39m total_samples\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_boxes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(557568, 256)  # Adjust input size based on image dimensions for image classification\n",
    "        self.fc2 = nn.Linear(256, num_boxes * 7)  # Each bounding box has 4 coordinates + num_classes for bounding box classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "       \n",
    "        x = x.view(x.size(0), -1) # Flatten the tensor\n",
    "        print(\"shape of x\",x.shape)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = x.view(x.size(0), -1, 7) # Reshape to (batch_size, num_boxes, 7)\n",
    "        return x\n",
    "# Define the transform to be applied to the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    # Add more transformations as needed\n",
    "])\n",
    "\n",
    "max_num_boxes=0\n",
    "# Calculate the maximum number of bounding boxes dynamically\n",
    "for images, targets in dataloader_train:\n",
    "    max_num_boxes = max(max_num_boxes, max(target.shape[0] for target in targets))\n",
    "\n",
    "\n",
    "# Initialize the model and other components\n",
    "model = SimpleCNN( num_boxes=max_num_boxes)  # Adjust num_classes according to your dataset\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Set the loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model for a few epochs (this is a simplified training loop)\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    total_samples = 0\n",
    "    correct_samples = 0\n",
    "    for images, targets in dataloader_train:\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Assuming targets is a list of your tensors\n",
    "        max_size = max(t.size(0) for t in targets)\n",
    "        targets = [F.pad(t, (0, 0, 0, max_size - t.size(0))) for t in targets]\n",
    "        targets = torch.stack(targets).to(device)\n",
    "        \n",
    "        # Re-initialize the optimizer\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Your logic for calculating accuracy may vary based on the structure of your targets\n",
    "        predictions = torch.sigmoid(outputs) > 0.5\n",
    "        correct_samples += (predictions == targets).sum().item()\n",
    "        total_samples += targets.numel()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    accuracy = 100.0 * correct_samples / total_samples\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}, Accuracy: {accuracy:.3f}%\")\n",
    "\n",
    "# Feature extraction for all images in dataloader_train\n",
    "with torch.no_grad():\n",
    "    all_extracted_features = []\n",
    "\n",
    "    for batch in dataloader_train:\n",
    "        images, _ = batch\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Assuming 'model' is your object detection model\n",
    "        extracted_features = model(images)\n",
    "        all_extracted_features.append(extracted_features)\n",
    "\n",
    "    # Concatenate features from all batches\n",
    "    all_extracted_features = torch.cat(all_extracted_features, dim=0)\n",
    "\n",
    "print(\"Shape of all extracted features:\", all_extracted_features.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### backbone model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.misc import NestedTensor, is_main_process\n",
    "\n",
    "import positional_encodding \n",
    "import resnetbackbone "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define your arguments\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.backbone = \"resnet50\"  # or any other supported backbone\n",
    "        self.lr_backbone = 0.1  # example learning rate for backbone, adjust as needed\n",
    "        self.masks = False  # whether to include masks in the output\n",
    "        self.num_feature_levels = 1  # number of feature levels, adjust as needed\n",
    "        self.dilation = False  # whether to use dilation in backbone\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# Build the backbone model\n",
    "model = resnetbackbone.build_backbone(args)\n",
    "\n",
    "# Assuming you have input data 'input_data', pass it through the model\n",
    "# input_data should be a NestedTensor, which is a tensor with an associated mask\n",
    "# Example usage assuming input_data is properly defined\n",
    "output = model(images)\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    all_extracted_features = []\n",
    "\n",
    "    for batch in dataloader_train:\n",
    "        images, _ = batch\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Assuming 'model' is your object detection model\n",
    "        extracted_features = model(images)\n",
    "        all_extracted_features.append(extracted_features)\n",
    "\n",
    "    # Concatenate features from all batches\n",
    "    all_extracted_features = torch.cat(all_extracted_features, dim=0)\n",
    "\n",
    "print(\"Shape of all extracted features:\", all_extracted_features.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
