{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "import torch.utils.data\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from pycocotools import mask as coco_mask\n",
    "\n",
    "import util.transforms as T\n",
    "\n",
    "import contextlib\n",
    "import copy\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import ToTensor, Resize, Compose, Normalize, RandomHorizontalFlip, RandomVerticalFlip, RandomRotation, RandomResizedCrop, ColorJitter\n",
    "import argparse, time, subprocess, io, shlex, pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from pycocotools.coco import COCO\n",
    "import pycocotools.mask as mask_util\n",
    "\n",
    "import util.misc as utils\n",
    "from util import box_ops\n",
    "from util.box_ops import box_cxcywh_to_xyxy, box_iou\n",
    "from util.utils import slprint, to_device\n",
    "\n",
    "import tqdm\n",
    "from itertools import islice\n",
    "\n",
    "from typing import Any, Tuple, List, Sequence, Dict, Iterable\n",
    "import torch.nn.functional as FF\n",
    "\n",
    "from resnetbackbone import build_backbone\n",
    "from util.misc import NestedTensor, nested_tensor_from_tensor_list, accuracy, get_world_size, is_dist_avail_and_initialized, interpolate, all_gather\n",
    "import matcher as matcherr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3060 Ti'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    raise SystemError('GPU device not found')\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "COCO dataset which returns image_id for evaluation.\n",
    "\n",
    "Mostly copy-paste from https://github.com/pytorch/vision/blob/13b35ff/references/detection/coco_utils.py\n",
    "\"\"\"\n",
    "if __name__==\"__main__\":\n",
    "    # for debug only\n",
    "    import os, sys\n",
    "    sys.path.append(os.path.dirname(sys.path[0]))\n",
    "\n",
    "__all__ = ['build']\n",
    "\n",
    "\n",
    "class CocoDetection(torchvision.datasets.CocoDetection):\n",
    "    def __init__(self, img_folder, ann_file, transforms, aux_target_hacks=None):\n",
    "        super(CocoDetection, self).__init__(img_folder, ann_file)\n",
    "        self._transforms = transforms\n",
    "        self.prepare = ConvertCocoPolysToMask()\n",
    "        self.aux_target_hacks = aux_target_hacks\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Output:\n",
    "            - target: dict of multiple items\n",
    "                - boxes: Tensor[num_box, 4]. \\\n",
    "                    Init type: x0,y0,x1,y1. unnormalized data.\n",
    "                    Final type: cx,cy,w,h. normalized data. \n",
    "        \"\"\"\n",
    "        try:\n",
    "            img, target = super(CocoDetection, self).__getitem__(idx)\n",
    "        except:\n",
    "            print(\"Error idx: {}\".format(idx))\n",
    "            idx += 1\n",
    "            img, target = super(CocoDetection, self).__getitem__(idx)\n",
    "        image_id = self.ids[idx]\n",
    "        target = {'image_id': image_id, 'annotations': target}\n",
    "        img, target = self.prepare(img, target)\n",
    "        \n",
    "        if self._transforms is not None:\n",
    "            img, target = self._transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "\n",
    "\"\"\"def convert_coco_poly_to_mask(segmentations, height, width):\n",
    "    masks = []\n",
    "    for polygons in segmentations:\n",
    "        rles = coco_mask.frPyObjects(polygons, height, width)\n",
    "        mask = coco_mask.decode(rles)\n",
    "        if len(mask.shape) < 3:\n",
    "            mask = mask[..., None]\n",
    "        mask = torch.as_tensor(mask, dtype=torch.uint8)\n",
    "        mask = mask.any(dim=2)\n",
    "        masks.append(mask)\n",
    "    if masks:\n",
    "        masks = torch.stack(masks, dim=0)\n",
    "    else:\n",
    "        masks = torch.zeros((0, height, width), dtype=torch.uint8)\n",
    "    return masks\"\"\"\n",
    "\n",
    "\n",
    "class ConvertCocoPolysToMask(object):\n",
    "    #def __init__(self, return_masks=False):\n",
    "        #self.return_masks = return_masks\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        w, h = image.size # extracts the width and height of the image.\n",
    "\n",
    "        image_id = target[\"image_id\"]\n",
    "        image_id = torch.tensor([image_id])\n",
    "\n",
    "        anno = target[\"annotations\"]\n",
    "\n",
    "        anno = [obj for obj in anno if 'iscrowd' not in obj or obj['iscrowd'] == 0]\n",
    "\n",
    "        boxes = [obj[\"bbox\"] for obj in anno]\n",
    "        # guard against no boxes via resizing\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32).reshape(-1, 4)\n",
    "        boxes[:, 2:] += boxes[:, :2]\n",
    "        boxes[:, 0::2].clamp_(min=0, max=w)\n",
    "        boxes[:, 1::2].clamp_(min=0, max=h)\n",
    "\n",
    "        classes = [obj[\"category_id\"] for obj in anno]\n",
    "        classes = torch.tensor(classes, dtype=torch.int64)\n",
    "\n",
    "        \"\"\"if self.return_masks:\n",
    "            segmentations = [obj[\"segmentation\"] for obj in anno]\n",
    "            masks = convert_coco_poly_to_mask(segmentations, h, w)\"\"\"\n",
    "\n",
    "        keypoints = None\n",
    "        if anno and \"keypoints\" in anno[0]:\n",
    "            keypoints = [obj[\"keypoints\"] for obj in anno]\n",
    "            keypoints = torch.as_tensor(keypoints, dtype=torch.float32)\n",
    "            num_keypoints = keypoints.shape[0]\n",
    "            if num_keypoints:\n",
    "                keypoints = keypoints.view(num_keypoints, -1, 3)\n",
    "\n",
    "        keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0])\n",
    "        boxes = boxes[keep]\n",
    "        classes = classes[keep]\n",
    "        #if self.return_masks:\n",
    "            #masks = masks[keep]\n",
    "        if keypoints is not None:\n",
    "            keypoints = keypoints[keep]\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = classes\n",
    "        #if self.return_masks:\n",
    "            #target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        if keypoints is not None:\n",
    "            target[\"keypoints\"] = keypoints\n",
    "\n",
    "        # for conversion to coco api\n",
    "        area = torch.tensor([obj[\"area\"] for obj in anno])\n",
    "        iscrowd = torch.tensor([obj[\"iscrowd\"] if \"iscrowd\" in obj else 0 for obj in anno])\n",
    "        target[\"area\"] = area[keep]\n",
    "        target[\"iscrowd\"] = iscrowd[keep]\n",
    "\n",
    "        target[\"orig_size\"] = torch.as_tensor([int(h), int(w)])\n",
    "        target[\"size\"] = torch.as_tensor([int(h), int(w)])\n",
    "\n",
    "        return image, target\n",
    "\n",
    "\n",
    "def make_coco_transforms(image_set, fix_size=False, strong_aug=False, args=None):\n",
    "\n",
    "    normalize = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # config the params for data aug\n",
    "    scales = [480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800]\n",
    "    max_size = 1333\n",
    "    scales2_resize = [400, 500, 600]\n",
    "    scales2_crop = [384, 600]\n",
    "    \n",
    "    # # update args from config files\n",
    "    # scales = getattr(args, 'data_aug_scales', scales)\n",
    "    # max_size = getattr(args, 'data_aug_max_size', max_size)\n",
    "    # scales2_resize = getattr(args, 'data_aug_scales2_resize', scales2_resize)\n",
    "    # scales2_crop = getattr(args, 'data_aug_scales2_crop', scales2_crop)\n",
    "\n",
    "    # # resize them\n",
    "    # data_aug_scale_overlap = getattr(args, 'data_aug_scale_overlap', None)\n",
    "    # if data_aug_scale_overlap is not None and data_aug_scale_overlap > 0:\n",
    "    #     data_aug_scale_overlap = float(data_aug_scale_overlap)\n",
    "    #     scales = [int(i*data_aug_scale_overlap) for i in scales]\n",
    "    #     max_size = int(max_size*data_aug_scale_overlap)\n",
    "    #     scales2_resize = [int(i*data_aug_scale_overlap) for i in scales2_resize]\n",
    "    #     scales2_crop = [int(i*data_aug_scale_overlap) for i in scales2_crop]\n",
    "\n",
    "\n",
    "    # datadict_for_print = {\n",
    "    #     'scales': scales,\n",
    "    #     'max_size': max_size,\n",
    "    #     'scales2_resize': scales2_resize,\n",
    "    #     'scales2_crop': scales2_crop\n",
    "    # }\n",
    "    # print(\"data_aug_params:\", json.dumps(datadict_for_print, indent=2))\n",
    "        \n",
    "\n",
    "    if image_set == 'train':\n",
    "        if fix_size:\n",
    "            return T.Compose([\n",
    "                T.RandomHorizontalFlip(),\n",
    "                T.RandomResize([(max_size, max(scales))]),\n",
    "                normalize,\n",
    "            ])\n",
    "\n",
    "        if strong_aug:\n",
    "            import util.sltransform as SLT\n",
    "            \n",
    "            return T.Compose([\n",
    "                T.RandomHorizontalFlip(),\n",
    "                T.RandomSelect(\n",
    "                    T.RandomResize(scales, max_size=max_size),\n",
    "                    T.Compose([\n",
    "                        T.RandomResize(scales2_resize),\n",
    "                        T.RandomSizeCrop(*scales2_crop),\n",
    "                        T.RandomResize(scales, max_size=max_size),\n",
    "                    ])\n",
    "                ),\n",
    "                SLT.RandomSelectMulti([\n",
    "                    SLT.RandomCrop(),\n",
    "                    # SLT.Rotate(10),\n",
    "                    SLT.LightingNoise(),\n",
    "                    SLT.AdjustBrightness(2),\n",
    "                    SLT.AdjustContrast(2),\n",
    "                ]),              \n",
    "                # # for debug only  \n",
    "                # SLT.RandomCrop(),\n",
    "                # SLT.LightingNoise(),\n",
    "                # SLT.AdjustBrightness(2),\n",
    "                # SLT.AdjustContrast(2),\n",
    "                # SLT.Rotate(10),\n",
    "                normalize,\n",
    "            ])\n",
    "        \n",
    "        return T.Compose([\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.RandomSelect(\n",
    "                T.RandomResize(scales, max_size=max_size),\n",
    "                T.Compose([\n",
    "                    T.RandomResize(scales2_resize),\n",
    "                    T.RandomSizeCrop(*scales2_crop),\n",
    "                    T.RandomResize(scales, max_size=max_size),\n",
    "                ])\n",
    "            ),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "    if image_set in ['val', 'test']:\n",
    "\n",
    "        if os.environ.get(\"GFLOPS_DEBUG_SHILONG\", False) == 'INFO':\n",
    "            print(\"Under debug mode for flops calculation only!!!!!!!!!!!!!!!!\")\n",
    "            return T.Compose([\n",
    "                T.ResizeDebug((1280, 800)),\n",
    "                normalize,\n",
    "            ])   \n",
    "\n",
    "        return T.Compose([\n",
    "            T.RandomResize([max(scales)], max_size=max_size),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "    raise ValueError(f'unknown {image_set}')\n",
    "\n",
    "\n",
    "def build(image_set, coco_path, strong_aug=False, fix_size=False):\n",
    "    root = Path(coco_path)\n",
    "    # assert root.exists(), f'provided COCO path {root} does not exist'\n",
    "    mode = 'instances'\n",
    "    PATHS = {\n",
    "        \"train\": (root / \"train2017\", root / \"annotations\" / f'{mode}_train2017.json'),\n",
    "        \"train_reg\": (root / \"train2017\", root / \"annotations\" / f'{mode}_train2017.json'),\n",
    "        \"val\": (root / \"val2017\", root / \"annotations\" / f'{mode}_val2017.json'),\n",
    "        \"eval_debug\": (root / \"val2017\", root / \"annotations\" / f'{mode}_val2017.json'),\n",
    "        \"test\": (root / \"test2017\", root / \"annotations\" / 'image_info_test-dev2017.json' ),\n",
    "    }\n",
    "\n",
    "    # add some hooks to datasets\n",
    "    aux_target_hacks_list = None\n",
    "    img_folder, ann_file = PATHS[image_set]\n",
    "\n",
    "    dataset = CocoDetection(img_folder, ann_file, \n",
    "            transforms=make_coco_transforms(image_set, fix_size=fix_size, strong_aug=strong_aug), \n",
    "            #return_masks=args.masks,\n",
    "            aux_target_hacks=aux_target_hacks_list,\n",
    "        )\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coco_api_from_dataset(dataset):\n",
    "    for _ in range(10):\n",
    "        # if isinstance(dataset, torchvision.datasets.CocoDetection):\n",
    "        #     break\n",
    "        if isinstance(dataset, torch.utils.data.Subset):\n",
    "            dataset = dataset.dataset\n",
    "    if isinstance(dataset, torchvision.datasets.CocoDetection):\n",
    "        return dataset.coco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize image dataset (don't run this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## show image (don't run this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COCO evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "COCO evaluator that works in distributed mode.\n",
    "\n",
    "Mostly copy-paste from https://github.com/pytorch/vision/blob/edfd5a7/references/detection/coco_eval.py\n",
    "The difference is that there is less copy-pasting from pycocotools\n",
    "in the end of the file, as python3 can suppress prints with contextlib\n",
    "\"\"\"\n",
    "\n",
    "class CocoEvaluator(object):\n",
    "    def __init__(self, coco_gt, iou_types, useCats=True):\n",
    "        assert isinstance(iou_types, (list, tuple))\n",
    "        coco_gt = copy.deepcopy(coco_gt)\n",
    "        self.coco_gt = coco_gt\n",
    "\n",
    "        self.iou_types = iou_types\n",
    "        self.coco_eval = {}  # is a dictionary where each key is an IoU type and each value is a COCOeval object\n",
    "        for iou_type in iou_types:\n",
    "            self.coco_eval[iou_type] = COCOeval(coco_gt, iouType=iou_type)\n",
    "            self.coco_eval[iou_type].useCats = useCats\n",
    "\n",
    "        self.img_ids = []\n",
    "        self.eval_imgs = {k: [] for k in iou_types}\n",
    "        self.useCats = useCats\n",
    "\n",
    "    def update(self, predictions):\n",
    "        img_ids = list(np.unique(list(predictions.keys())))\n",
    "        self.img_ids.extend(img_ids)\n",
    "\n",
    "        for iou_type in self.iou_types:\n",
    "            results = self.prepare(predictions, iou_type)\n",
    "\n",
    "            # suppress pycocotools prints\n",
    "            with open(os.devnull, 'w') as devnull:\n",
    "                with contextlib.redirect_stdout(devnull):\n",
    "                    coco_dt = COCO.loadRes(self.coco_gt, results) if results else COCO()\n",
    "            coco_eval = self.coco_eval[iou_type]\n",
    "\n",
    "            coco_eval.cocoDt = coco_dt\n",
    "            coco_eval.params.imgIds = list(img_ids)\n",
    "            coco_eval.params.useCats = self.useCats\n",
    "            img_ids, eval_imgs = evaluate(coco_eval)\n",
    "\n",
    "            self.eval_imgs[iou_type].append(eval_imgs)\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        for iou_type in self.iou_types:\n",
    "            self.eval_imgs[iou_type] = np.concatenate(self.eval_imgs[iou_type], 2)\n",
    "            create_common_coco_eval(self.coco_eval[iou_type], self.img_ids, self.eval_imgs[iou_type])\n",
    "\n",
    "    def accumulate(self):\n",
    "        for coco_eval in self.coco_eval.values():\n",
    "            coco_eval.accumulate()\n",
    "\n",
    "    def summarize(self):\n",
    "        for iou_type, coco_eval in self.coco_eval.items():\n",
    "            print(\"IoU metric: {}\".format(iou_type))\n",
    "            coco_eval.summarize()\n",
    "\n",
    "    def prepare(self, predictions, iou_type):\n",
    "        if iou_type == \"bbox\":\n",
    "            return self.prepare_for_coco_detection(predictions)\n",
    "        #elif iou_type == \"segm\":\n",
    "            #return self.prepare_for_coco_segmentation(predictions)\n",
    "        elif iou_type == \"keypoints\":\n",
    "            return self.prepare_for_coco_keypoint(predictions)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown iou type {}\".format(iou_type))\n",
    "\n",
    "    def prepare_for_coco_detection(self, predictions):\n",
    "        coco_results = []\n",
    "        for original_id, prediction in predictions.items():\n",
    "            if len(prediction) == 0:\n",
    "                continue\n",
    "\n",
    "            boxes = prediction[\"boxes\"]\n",
    "            boxes = convert_to_xywh(boxes).tolist()\n",
    "            if not isinstance(prediction[\"scores\"], list):\n",
    "                scores = prediction[\"scores\"].tolist()\n",
    "            else:\n",
    "                scores = prediction[\"scores\"]\n",
    "            if not isinstance(prediction[\"labels\"], list):\n",
    "                labels = prediction[\"labels\"].tolist()\n",
    "            else:\n",
    "                labels = prediction[\"labels\"]\n",
    "\n",
    "        \n",
    "            try:\n",
    "                coco_results.extend(\n",
    "                    [\n",
    "                        {\n",
    "                            \"image_id\": original_id,\n",
    "                            \"category_id\": labels[k],\n",
    "                            \"bbox\": box,\n",
    "                            \"score\": scores[k],\n",
    "                        }\n",
    "                        for k, box in enumerate(boxes)\n",
    "                    ]\n",
    "                )\n",
    "            except:\n",
    "                import ipdb; \n",
    "                ipdb.set_trace()\n",
    "        return coco_results\n",
    "\n",
    "    \"\"\"def prepare_for_coco_segmentation(self, predictions):\n",
    "        coco_results = []\n",
    "        for original_id, prediction in predictions.items():\n",
    "            if len(prediction) == 0:\n",
    "                continue\n",
    "\n",
    "            scores = prediction[\"scores\"]\n",
    "            labels = prediction[\"labels\"]\n",
    "            masks = prediction[\"masks\"]\n",
    "\n",
    "            masks = masks > 0.5\n",
    "\n",
    "            scores = prediction[\"scores\"].tolist()\n",
    "            labels = prediction[\"labels\"].tolist()\n",
    "\n",
    "            rles = [\n",
    "                mask_util.encode(np.array(mask[0, :, :, np.newaxis], dtype=np.uint8, order=\"F\"))[0]\n",
    "                for mask in masks\n",
    "            ]\n",
    "            for rle in rles:\n",
    "                rle[\"counts\"] = rle[\"counts\"].decode(\"utf-8\")\n",
    "\n",
    "            coco_results.extend(\n",
    "                [\n",
    "                    {\n",
    "                        \"image_id\": original_id,\n",
    "                        \"category_id\": labels[k],\n",
    "                        \"segmentation\": rle,\n",
    "                        \"score\": scores[k],\n",
    "                    }\n",
    "                    for k, rle in enumerate(rles)\n",
    "                ]\n",
    "            )\n",
    "        return coco_results\"\"\"\n",
    "\n",
    "    def prepare_for_coco_keypoint(self, predictions):\n",
    "        coco_results = []\n",
    "        for original_id, prediction in predictions.items():\n",
    "            if len(prediction) == 0:\n",
    "                continue\n",
    "\n",
    "            boxes = prediction[\"boxes\"]\n",
    "            boxes = convert_to_xywh(boxes).tolist()\n",
    "            scores = prediction[\"scores\"].tolist()\n",
    "            labels = prediction[\"labels\"].tolist()\n",
    "            keypoints = prediction[\"keypoints\"]\n",
    "            keypoints = keypoints.flatten(start_dim=1).tolist()\n",
    "\n",
    "            coco_results.extend(\n",
    "                [\n",
    "                    {\n",
    "                        \"image_id\": original_id,\n",
    "                        \"category_id\": labels[k],\n",
    "                        'keypoints': keypoint,\n",
    "                        \"score\": scores[k],\n",
    "                    }\n",
    "                    for k, keypoint in enumerate(keypoints)\n",
    "                ]\n",
    "            )\n",
    "        return coco_results\n",
    "\n",
    "\n",
    "def convert_to_xywh(boxes):\n",
    "    xmin, ymin, xmax, ymax = boxes.unbind(1)\n",
    "    return torch.stack((xmin, ymin, xmax - xmin, ymax - ymin), dim=1)\n",
    "\n",
    "\n",
    "def merge(img_ids, eval_imgs):\n",
    "    all_img_ids = all_gather(img_ids)\n",
    "    all_eval_imgs = all_gather(eval_imgs)\n",
    "\n",
    "    merged_img_ids = []\n",
    "    for p in all_img_ids:\n",
    "        merged_img_ids.extend(p)\n",
    "\n",
    "    merged_eval_imgs = []\n",
    "    for p in all_eval_imgs:\n",
    "        merged_eval_imgs.append(p)\n",
    "\n",
    "    merged_img_ids = np.array(merged_img_ids)\n",
    "    merged_eval_imgs = np.concatenate(merged_eval_imgs, 2)\n",
    "\n",
    "    # keep only unique (and in sorted order) images\n",
    "    merged_img_ids, idx = np.unique(merged_img_ids, return_index=True)\n",
    "    merged_eval_imgs = merged_eval_imgs[..., idx]\n",
    "\n",
    "    return merged_img_ids, merged_eval_imgs\n",
    "\n",
    "\n",
    "def create_common_coco_eval(coco_eval, img_ids, eval_imgs):\n",
    "    img_ids, eval_imgs = merge(img_ids, eval_imgs)\n",
    "    img_ids = list(img_ids)\n",
    "    eval_imgs = list(eval_imgs.flatten())\n",
    "\n",
    "    coco_eval.evalImgs = eval_imgs\n",
    "    coco_eval.params.imgIds = img_ids\n",
    "    coco_eval._paramsEval = copy.deepcopy(coco_eval.params)\n",
    "\n",
    "\n",
    "#################################################################\n",
    "# From pycocotools, just removed the prints and fixed\n",
    "# a Python3 bug about unicode not defined\n",
    "#################################################################\n",
    "\n",
    "\n",
    "def evaluate(self):\n",
    "    '''\n",
    "    Run per image evaluation on given images and store results (a list of dict) in self.evalImgs\n",
    "    :return: None\n",
    "    '''\n",
    "    # tic = time.time()\n",
    "    # print('Running per image evaluation...')\n",
    "    p = self.params #  retrieving the evaluation parameters stored in self.params\n",
    "    # add backward compatibility if useSegm is specified in params\n",
    "    if p.useSegm is not None:\n",
    "        p.iouType = 'segm' if p.useSegm == 1 else 'bbox'\n",
    "        print('useSegm (deprecated) is not None. Running {} evaluation'.format(p.iouType))\n",
    "    # print('Evaluate annotation type *{}*'.format(p.iouType))\n",
    "    p.imgIds = list(np.unique(p.imgIds))\n",
    "    if p.useCats:\n",
    "        p.catIds = list(np.unique(p.catIds))\n",
    "    p.maxDets = sorted(p.maxDets)\n",
    "    self.params = p\n",
    "\n",
    "    self._prepare()\n",
    "    # loop through images, area range, max detection number\n",
    "    catIds = p.catIds if p.useCats else [-1]\n",
    "\n",
    "    if p.iouType == 'segm' or p.iouType == 'bbox':\n",
    "        computeIoU = self.computeIoU\n",
    "    elif p.iouType == 'keypoints':\n",
    "        computeIoU = self.computeOks\n",
    "    self.ious = {\n",
    "        (imgId, catId): computeIoU(imgId, catId)\n",
    "        for imgId in p.imgIds\n",
    "        for catId in catIds}\n",
    "\n",
    "    evaluateImg = self.evaluateImg\n",
    "    maxDet = p.maxDets[-1]\n",
    "    evalImgs = [\n",
    "        evaluateImg(imgId, catId, areaRng, maxDet)\n",
    "        for catId in catIds\n",
    "        for areaRng in p.areaRng\n",
    "        for imgId in p.imgIds\n",
    "    ]\n",
    "    # this is NOT in the pycocotools code, but could be done outside\n",
    "    evalImgs = np.asarray(evalImgs).reshape(len(catIds), len(p.areaRng), len(p.imgIds))\n",
    "    self._paramsEval = copy.deepcopy(self.params)\n",
    "    toc = time.time()\n",
    "    print('DONE (t={:0.2f}s).'.format(toc-tic))\n",
    "    return p.imgIds, evalImgs\n",
    "\n",
    "#################################################################\n",
    "# end of straight copy from pycocotools, just removing the prints\n",
    "#################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backbone Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    raise SystemError('GPU device not found')\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### backbone model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from resnetbackbone import build_backbone\n",
    "from util.misc import NestedTensor, nested_tensor_from_tensor_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Pretrain resnet50 Model (don't run this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape All Extracted Feature (don't run this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DETR model ( From DAB detr model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import ipykernel\n",
    "\n",
    "import import_ipynb\n",
    "from typing import List, Tuple, Dict\n",
    "import copy\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "\n",
    "import warnings\n",
    "from typing import Optional, Tuple\n",
    "from torch.nn.modules.linear import Linear\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.nn.init import constant_\n",
    "from torch.nn.init import xavier_normal_\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "import math\n",
    "\n",
    "from torch._C import _infer_size, _add_docstr\n",
    "from torch.nn import _reduction as _Reduction\n",
    "from torch.nn.modules import utils\n",
    "from torch.nn.modules.utils import _single, _pair, _triple, _list_with_default\n",
    "from torch.nn import grad\n",
    "from torch import _VF\n",
    "from torch._jit_internal import boolean_dispatch, List, Optional, Tuple, ignore, _overload\n",
    "try:\n",
    "    from torch.overrides import has_torch_function, handle_torch_function\n",
    "except:\n",
    "    from torch.overrides import has_torch_function, handle_torch_function\n",
    "Tensor = torch.Tensor\n",
    "\n",
    "from torch.nn.functional import linear,pad,relu,softmax,dropout   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDetr(nn.Module):\n",
    "    \n",
    "## positional embedding\n",
    "    @staticmethod\n",
    "    def position_embedding_sine(tensor_list, num_pos_feats=64, temperature=10000, normalize=False, scale=None):\n",
    "        x = tensor_list.tensors\n",
    "        print(f\"x:{x}\")\n",
    "    \n",
    "        mask = tensor_list.mask\n",
    "        assert mask is not None\n",
    "        not_mask = ~mask\n",
    "        y_embed = not_mask.cumsum(1, dtype=torch.float32)\n",
    "        x_embed = not_mask.cumsum(2, dtype=torch.float32)\n",
    "        if normalize:\n",
    "            eps = 1e-6\n",
    "            y_embed = y_embed / (y_embed[:, -1:, :] + eps) * (2 * math.pi)\n",
    "            x_embed = x_embed / (x_embed[:, :, -1:] + eps) * (2 * math.pi)\n",
    "\n",
    "        dim_t = torch.arange(num_pos_feats, dtype=torch.float32, device=x.device)\n",
    "        dim_t = temperature ** (2 * (dim_t // 2) / num_pos_feats)\n",
    "\n",
    "        pos_x = x_embed[:, :, :, None] / dim_t\n",
    "        pos_y = y_embed[:, :, :, None] / dim_t\n",
    "        pos_x = torch.stack((pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
    "        pos_y = torch.stack((pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
    "        pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)\n",
    "        return pos\n",
    "    \n",
    "    @staticmethod\n",
    "    def position_embedding_learned(tensor_list, num_pos_feats=256):\n",
    "        x = tensor_list.tensors\n",
    "        h, w = x.shape[-2:]\n",
    "        i = torch.arange(w, device=x.device)\n",
    "        j = torch.arange(h, device=x.device)\n",
    "        \n",
    "        i = i.clamp(max=49)\n",
    "        j = j.clamp(max=49)\n",
    "        \n",
    "        x_emb = nn.Embedding(50, num_pos_feats)(i)\n",
    "        y_emb = nn.Embedding(50, num_pos_feats)(j)\n",
    "        pos = torch.cat([\n",
    "            x_emb.unsqueeze(0).repeat(h, 1, 1),\n",
    "            y_emb.unsqueeze(1).repeat(1, w, 1),\n",
    "        ], dim=-1).permute(2, 0, 1).unsqueeze(0).repeat(x.shape[0], 1, 1, 1)\n",
    "        return pos\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def build_position_encoding(position_embedding, hidden_dim=256):\n",
    "        N_steps = hidden_dim // 2\n",
    "        if position_embedding in ('v2', 'sine'):\n",
    "            position_embedding = SimpleDetr.position_embedding_sine\n",
    "        elif position_embedding in ('v3', 'learned'):\n",
    "            position_embedding = SimpleDetr.position_embedding_learned\n",
    "        else:\n",
    "            raise ValueError(f\"nott supported {position_embedding}\")\n",
    "\n",
    "        return position_embedding\n",
    "    \n",
    "\n",
    "def gen_sineembed_for_position(pos_tensor, d_model=256):\n",
    "    # n_query, bs, _ = pos_tensor.size()\n",
    "    # sineembed_tensor = torch.zeros(n_query, bs, 256)\n",
    "    scale = 2 * math.pi\n",
    "    dim_t = torch.arange(d_model // 2, dtype=torch.float32, device=pos_tensor.device)\n",
    "    dim_t = 10000 ** (2 * (dim_t // 2) / (d_model // 2))\n",
    "    \n",
    "    x_embed = pos_tensor[:, :, 0] * scale\n",
    "    y_embed = pos_tensor[:, :, 1] * scale\n",
    "    \n",
    "    pos_x = x_embed[:, :, None] / dim_t\n",
    "    pos_y = y_embed[:, :, None] / dim_t\n",
    "    pos_x = torch.stack((pos_x[:, :, 0::2].sin(), pos_x[:, :, 1::2].cos()), dim=3).flatten(2)\n",
    "    pos_y = torch.stack((pos_y[:, :, 0::2].sin(), pos_y[:, :, 1::2].cos()), dim=3).flatten(2)\n",
    "    if pos_tensor.size(-1) == 2:\n",
    "        pos = torch.cat((pos_y, pos_x), dim=2)\n",
    "    elif pos_tensor.size(-1) == 4:\n",
    "        w_embed = pos_tensor[:, :, 2] * scale\n",
    "        pos_w = w_embed[:, :, None] / dim_t\n",
    "        pos_w = torch.stack((pos_w[:, :, 0::2].sin(), pos_w[:, :, 1::2].cos()), dim=3).flatten(2)\n",
    "\n",
    "        h_embed = pos_tensor[:, :, 3] * scale\n",
    "        pos_h = h_embed[:, :, None] / dim_t\n",
    "        pos_h = torch.stack((pos_h[:, :, 0::2].sin(), pos_h[:, :, 1::2].cos()), dim=3).flatten(2)\n",
    "\n",
    "        pos = torch.cat((pos_y, pos_x, pos_w, pos_h), dim=2)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown pos_tensor shape(-1):{}\".format(pos_tensor.size(-1)))\n",
    "    return pos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed forward Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module): # Multi Layer Perceptron (MLP) for the feedforward network\n",
    "\n",
    "    def __init__(self, d_model: int, hidden_dim: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, hidden_dim) # w1 and b1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(hidden_dim, d_model) # w2 and b2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, hidden_dim) --> (batch, seq_len, d_model)\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\" Very simple multi-layer perceptron (also called FFN)\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        h = [hidden_dim] * (num_layers - 1)\n",
    "        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiHeadAttentionBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(Module):\n",
    "\n",
    "    bias_k: Optional[torch.Tensor]\n",
    "    bias_v: Optional[torch.Tensor]\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0., bias=True, add_bias_kv=False, add_zero_attn=False, kdim=None, vdim=None):\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.kdim = kdim if kdim is not None else embed_dim\n",
    "        self.vdim = vdim if vdim is not None else embed_dim\n",
    "        self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        print(f\"embed_dim:{self.embed_dim}\")\n",
    "        print(f\"num_heads:{self.num_heads}\")\n",
    "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        vdim = vdim if vdim is not None else embed_dim\n",
    "        self.out_proj = Linear(vdim , vdim)\n",
    "\n",
    "        self.in_proj_bias = None\n",
    "        self.in_proj_weight = None\n",
    "        self.bias_k = self.bias_v = None\n",
    "        self.q_proj_weight = None\n",
    "        self.k_proj_weight = None\n",
    "        self.v_proj_weight = None\n",
    "\n",
    "        self.add_zero_attn = add_zero_attn\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        constant_(self.out_proj.bias, 0.)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        # Support loading old MultiheadAttention checkpoints generated by v1.1.0\n",
    "        if '_qkv_same_embed_dim' not in state:\n",
    "            state['_qkv_same_embed_dim'] = True\n",
    "\n",
    "        super(MultiheadAttention, self).__setstate__(state)\n",
    "\n",
    "    def forward(self, query, key, value, key_padding_mask=None,\n",
    "                need_weights=True, attn_mask=None):\n",
    "        # type: (Tensor, Tensor, Tensor, Optional[Tensor], bool, Optional[Tensor]) -> Tuple[Tensor, Optional[Tensor]]\n",
    "\n",
    "\n",
    "        if not self._qkv_same_embed_dim:\n",
    "            return multi_head_attention_forward(\n",
    "                query, key, value, self.embed_dim, self.num_heads,\n",
    "                self.in_proj_weight, self.in_proj_bias,\n",
    "                self.bias_k, self.bias_v, self.add_zero_attn,\n",
    "                self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
    "                training=self.training,\n",
    "                key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
    "                attn_mask=attn_mask, use_separate_proj_weight=True,\n",
    "                q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,\n",
    "                v_proj_weight=self.v_proj_weight, out_dim=self.vdim)\n",
    "        else:\n",
    "            return multi_head_attention_forward(\n",
    "                query, key, value, self.embed_dim, self.num_heads,\n",
    "                self.in_proj_weight, self.in_proj_bias,\n",
    "                self.bias_k, self.bias_v, self.add_zero_attn,\n",
    "                self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
    "                training=self.training,\n",
    "                key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
    "                attn_mask=attn_mask, out_dim=self.vdim)\n",
    "\n",
    "\n",
    "def multi_head_attention_forward(query: Tensor,\n",
    "                                 key: Tensor,\n",
    "                                 value: Tensor,\n",
    "                                 embed_dim_to_check: int,\n",
    "                                 num_heads: int,\n",
    "                                 in_proj_weight: Tensor,\n",
    "                                 in_proj_bias: Tensor,\n",
    "                                 bias_k: Optional[Tensor],\n",
    "                                 bias_v: Optional[Tensor],\n",
    "                                 add_zero_attn: bool,\n",
    "                                 dropout_p: float,\n",
    "                                 out_proj_weight: Tensor,\n",
    "                                 out_proj_bias: Tensor,\n",
    "                                 training: bool = True,\n",
    "                                 key_padding_mask: Optional[Tensor] = None,\n",
    "                                 need_weights: bool = True,\n",
    "                                 attn_mask: Optional[Tensor] = None,\n",
    "                                 use_separate_proj_weight: bool = False,\n",
    "                                 q_proj_weight: Optional[Tensor] = None,\n",
    "                                 k_proj_weight: Optional[Tensor] = None,\n",
    "                                 v_proj_weight: Optional[Tensor] = None,\n",
    "                                 static_k: Optional[Tensor] = None,\n",
    "                                 static_v: Optional[Tensor] = None,\n",
    "                                 out_dim: Optional[Tensor] = None\n",
    "                                 ) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "    \n",
    "    \n",
    "    if not torch.jit.is_scripting():\n",
    "        tens_ops = (query, key, value, in_proj_weight, in_proj_bias, bias_k, bias_v,\n",
    "                    out_proj_weight, out_proj_bias)\n",
    "        if any([type(t) is not Tensor for t in tens_ops]) and has_torch_function(tens_ops):\n",
    "            return handle_torch_function(\n",
    "                multi_head_attention_forward, tens_ops, query, key, value,\n",
    "                embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias,\n",
    "                bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight,\n",
    "                out_proj_bias, training=training, key_padding_mask=key_padding_mask,\n",
    "                need_weights=need_weights, attn_mask=attn_mask,\n",
    "                use_separate_proj_weight=use_separate_proj_weight,\n",
    "                q_proj_weight=q_proj_weight, k_proj_weight=k_proj_weight,\n",
    "                v_proj_weight=v_proj_weight, static_k=static_k, static_v=static_v)\n",
    "    tgt_len, bsz, embed_dim = query.size()\n",
    "    assert embed_dim == embed_dim_to_check\n",
    "    # allow MHA to have different sizes for the feature dimension\n",
    "    assert key.size(0) == value.size(0) and key.size(1) == value.size(1)\n",
    "\n",
    "    head_dim = embed_dim // num_heads\n",
    "    v_head_dim = out_dim // num_heads\n",
    "    assert head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "    scaling = float(head_dim) ** -0.5\n",
    "\n",
    "    q = query * scaling\n",
    "    k = key\n",
    "    v = value\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        assert attn_mask.dtype == torch.float32 or attn_mask.dtype == torch.float64 or \\\n",
    "            attn_mask.dtype == torch.float16 or attn_mask.dtype == torch.uint8 or attn_mask.dtype == torch.bool, \\\n",
    "            'Only float, byte, and bool types are supported for attn_mask, not {}'.format(attn_mask.dtype)\n",
    "        if attn_mask.dtype == torch.uint8:\n",
    "            warnings.warn(\"Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\")\n",
    "            attn_mask = attn_mask.to(torch.bool)\n",
    "\n",
    "        if attn_mask.dim() == 2:\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "            if list(attn_mask.size()) != [1, query.size(0), key.size(0)]:\n",
    "                raise RuntimeError('The size of the 2D attn_mask is not correct.')\n",
    "        elif attn_mask.dim() == 3:\n",
    "            if list(attn_mask.size()) != [bsz * num_heads, query.size(0), key.size(0)]: # bsz is batch size\n",
    "                raise RuntimeError('The size of the 3D attn_mask is not correct.')\n",
    "        else:\n",
    "            raise RuntimeError(\"attn_mask's dimension {} is not supported\".format(attn_mask.dim()))\n",
    "        # attn_mask's dim is 3 now.\n",
    "\n",
    "    # convert ByteTensor key_padding_mask to bool\n",
    "    if key_padding_mask is not None and key_padding_mask.dtype == torch.uint8:\n",
    "        warnings.warn(\"Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\")\n",
    "        key_padding_mask = key_padding_mask.to(torch.bool)\n",
    "\n",
    "    if bias_k is not None and bias_v is not None:\n",
    "        if static_k is None and static_v is None:\n",
    "            k = torch.cat([k, bias_k.repeat(1, bsz, 1)])\n",
    "            v = torch.cat([v, bias_v.repeat(1, bsz, 1)])\n",
    "            if attn_mask is not None:\n",
    "                attn_mask = pad(attn_mask, (0, 1))\n",
    "            if key_padding_mask is not None:\n",
    "                key_padding_mask = pad(key_padding_mask, (0, 1))\n",
    "        else:\n",
    "            assert static_k is None, \"bias cannot be added to static key.\"\n",
    "            assert static_v is None, \"bias cannot be added to static value.\"\n",
    "    else:\n",
    "        assert bias_k is None\n",
    "        assert bias_v is None\n",
    "        \n",
    " # reshaping the input to be compatible with the linear projection\n",
    "    q = q.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1) \n",
    "    if k is not None:\n",
    "        k = k.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)  # bsz is batch size\n",
    "    if v is not None:\n",
    "        v = v.contiguous().view(-1, bsz * num_heads, v_head_dim).transpose(0, 1)\n",
    "\n",
    "    if static_k is not None:\n",
    "        assert static_k.size(0) == bsz * num_heads\n",
    "        assert static_k.size(2) == head_dim\n",
    "        k = static_k\n",
    "\n",
    "    if static_v is not None:\n",
    "        assert static_v.size(0) == bsz * num_heads # bsz is batch size\n",
    "        assert static_v.size(2) == v_head_dim\n",
    "        v = static_v\n",
    "\n",
    "    src_len = k.size(1)\n",
    "\n",
    "    if key_padding_mask is not None:\n",
    "        assert key_padding_mask.size(0) == bsz  # bsz is batch size\n",
    "        assert key_padding_mask.size(1) == src_len\n",
    "\n",
    "    if add_zero_attn:\n",
    "        src_len += 1\n",
    "        k = torch.cat([k, torch.zeros((k.size(0), 1) + k.size()[2:], dtype=k.dtype, device=k.device)], dim=1)\n",
    "        v = torch.cat([v, torch.zeros((v.size(0), 1) + v.size()[2:], dtype=v.dtype, device=v.device)], dim=1)\n",
    "        if attn_mask is not None:\n",
    "            attn_mask = pad(attn_mask, (0, 1))\n",
    "        if key_padding_mask is not None:\n",
    "            key_padding_mask = pad(key_padding_mask, (0, 1))\n",
    "\n",
    "    attn_output_weights = torch.bmm(q, k.transpose(1, 2)) # batch matrix multiplication (torch.bmm) q and k\n",
    "    assert list(attn_output_weights.size()) == [bsz * num_heads, tgt_len, src_len]\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.dtype == torch.bool:\n",
    "            attn_output_weights.masked_fill_(attn_mask, float('-inf'))\n",
    "        else:\n",
    "            attn_output_weights += attn_mask\n",
    "\n",
    "\n",
    "    if key_padding_mask is not None:\n",
    "        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)\n",
    "        attn_output_weights = attn_output_weights.masked_fill(\n",
    "            key_padding_mask.unsqueeze(1).unsqueeze(2),\n",
    "            float('-inf'),\n",
    "        )\n",
    "        attn_output_weights = attn_output_weights.view(bsz * num_heads, tgt_len, src_len)\n",
    "\n",
    "    # attn_output_weights = softmax(\n",
    "    #     attn_output_weights, dim=-1)\n",
    "    attn_output_weights = softmax(\n",
    "            attn_output_weights - attn_output_weights.max(dim=-1, keepdim=True)[0], dim=-1)\n",
    "    attn_output_weights = dropout(attn_output_weights, p=dropout_p, training=training)\n",
    "\n",
    "    attn_output = torch.bmm(attn_output_weights, v) # batch matrix multiplication (torch.bmm) with v\n",
    "    assert list(attn_output.size()) == [bsz * num_heads, tgt_len, v_head_dim]\n",
    "    attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, out_dim)\n",
    "    attn_output = linear(attn_output, out_proj_weight, out_proj_bias)\n",
    "\n",
    "    if need_weights:\n",
    "        # average attention weights over heads\n",
    "        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)\n",
    "        return attn_output, attn_output_weights.sum(dim=1) / num_heads\n",
    "    else:\n",
    "        return attn_output, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary functions for the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "def inverse_sigmoid(x, eps=1e-3):\n",
    "    x = x.clamp(min=0, max=1)\n",
    "    x1 = x.clamp(min=eps)\n",
    "    x2 = (1 - x).clamp(min=eps)\n",
    "    return torch.log(x1/x2)\n",
    "\n",
    "def _get_activation_fn(activation):\n",
    "    \"\"\"Return an activation function given a string\"\"\"\n",
    "    if activation == \"relu\":\n",
    "        return F.relu\n",
    "    if activation == \"gelu\":\n",
    "        return F.gelu\n",
    "    if activation == \"glu\":\n",
    "        return F.glu\n",
    "    if activation == \"prelu\":\n",
    "        return nn.PReLU()\n",
    "    if activation == \"selu\":\n",
    "        return F.selu\n",
    "    raise RuntimeError(F\"activation should be relu/gelu, not {activation}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder_layer, num_layers, norm=None, d_model=256):\n",
    "        super().__init__()\n",
    "        self.layers = _get_clones(encoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.query_scale = MLP(d_model, d_model, d_model, 2)\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, src,\n",
    "                mask: Optional[Tensor] = None,\n",
    "                src_key_padding_mask: Optional[Tensor] = None,\n",
    "                pos: Optional[Tensor] = None):\n",
    "        output = src\n",
    "\n",
    "        for layer_id, layer in enumerate(self.layers):\n",
    "            # rescale the content and pos sim\n",
    "            pos_scales = self.query_scale(output)\n",
    "            output = layer(output, src_mask=mask,\n",
    "                           src_key_padding_mask=src_key_padding_mask, pos=pos*pos_scales)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
    "                 activation=\"relu\", normalize_before=False):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "        self.normalize_before = normalize_before\n",
    "\n",
    "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
    "        return tensor if pos is None else tensor + pos\n",
    "\n",
    "    def forward(self,\n",
    "                     src,\n",
    "                     src_mask: Optional[Tensor] = None,\n",
    "                     src_key_padding_mask: Optional[Tensor] = None,\n",
    "                     pos: Optional[Tensor] = None):\n",
    "        q = k = self.with_pos_embed(src, pos)\n",
    "        src2 = self.self_attn(q, k, value=src, attn_mask=src_mask,\n",
    "                              key_padding_mask=src_key_padding_mask)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize encoder layer output (don't run this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, decoder_layer, num_layers, norm=None, return_intermediate=False, \n",
    "                    d_model=256, query_dim=2, keep_query_pos=False, query_scale_type='cond_elewise',\n",
    "                    modulate_hw_attn=False,\n",
    "                    bbox_embed_diff_each_layer=False,\n",
    "                    ):\n",
    "        super().__init__()\n",
    "        self.layers = _get_clones(decoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "        self.return_intermediate = return_intermediate\n",
    "        assert return_intermediate\n",
    "        self.query_dim = query_dim\n",
    "\n",
    "        assert query_scale_type in ['cond_elewise', 'cond_scalar', 'fix_elewise']\n",
    "        self.query_scale_type = query_scale_type\n",
    "        if query_scale_type == 'cond_elewise':\n",
    "            self.query_scale = MLP(d_model, d_model, d_model, 2)\n",
    "        elif query_scale_type == 'cond_scalar':\n",
    "            self.query_scale = MLP(d_model, d_model, 1, 2)\n",
    "        elif query_scale_type == 'fix_elewise':\n",
    "            self.query_scale = nn.Embedding(num_layers, d_model)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Unknown query_scale_type: {}\".format(query_scale_type))\n",
    "        \n",
    "        self.ref_point_head = MLP(query_dim // 2 * d_model, d_model, d_model, 2)\n",
    "        \n",
    "        self.bbox_embed = None\n",
    "        self.d_model = d_model\n",
    "        self.modulate_hw_attn = modulate_hw_attn\n",
    "        self.bbox_embed_diff_each_layer = bbox_embed_diff_each_layer\n",
    "\n",
    "\n",
    "        if modulate_hw_attn:\n",
    "            self.ref_anchor_head = MLP(d_model, d_model, 2, 2)\n",
    "\n",
    "        \n",
    "        if not keep_query_pos:\n",
    "            for layer_id in range(num_layers - 1):\n",
    "                self.layers[layer_id + 1].ca_qpos_proj = None\n",
    "\n",
    "    def forward(self, tgt, memory,\n",
    "                tgt_mask: Optional[Tensor] = None,\n",
    "                memory_mask: Optional[Tensor] = None,\n",
    "                tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                memory_key_padding_mask: Optional[Tensor] = None,\n",
    "                pos: Optional[Tensor] = None,\n",
    "                refpoints_unsigmoid: Optional[Tensor] = None, # num_queries, bs, 2\n",
    "                ):\n",
    "        output = tgt\n",
    "\n",
    "        intermediate = []\n",
    "        reference_points = refpoints_unsigmoid.sigmoid()\n",
    "        ref_points = [reference_points]\n",
    "\n",
    "        # import ipdb; ipdb.set_trace()        \n",
    "\n",
    "        for layer_id, layer in enumerate(self.layers):\n",
    "            obj_center = reference_points[..., :self.query_dim]     # [num_queries, batch_size, 2]\n",
    "            # get sine embedding for the query vector\n",
    "            query_sine_embed = gen_sineembed_for_position(obj_center, self.d_model)  \n",
    "            query_pos = self.ref_point_head(query_sine_embed) \n",
    "\n",
    "            # For the first decoder layer, we do not apply transformation over p_s\n",
    "            if self.query_scale_type != 'fix_elewise':\n",
    "                if layer_id == 0:\n",
    "                    pos_transformation = 1\n",
    "                else:\n",
    "                    pos_transformation = self.query_scale(output)\n",
    "            else:\n",
    "                pos_transformation = self.query_scale.weight[layer_id]\n",
    "\n",
    "            # apply transformation\n",
    "            query_sine_embed = query_sine_embed[...,:self.d_model] * pos_transformation\n",
    "\n",
    "            # modulated HW attentions\n",
    "            if self.modulate_hw_attn:\n",
    "                refHW_cond = self.ref_anchor_head(output).sigmoid() # nq, bs, 2\n",
    "                query_sine_embed[..., self.d_model // 2:] *= (refHW_cond[..., 0] / obj_center[..., 2]).unsqueeze(-1)\n",
    "                query_sine_embed[..., :self.d_model // 2] *= (refHW_cond[..., 1] / obj_center[..., 3]).unsqueeze(-1)\n",
    "\n",
    "\n",
    "            output = layer(output, memory, tgt_mask=tgt_mask,\n",
    "                           memory_mask=memory_mask,\n",
    "                           tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                           memory_key_padding_mask=memory_key_padding_mask,\n",
    "                           pos=pos, query_pos=query_pos, query_sine_embed=query_sine_embed,\n",
    "                           is_first=(layer_id == 0))\n",
    "\n",
    "            # iter update\n",
    "            if self.bbox_embed is not None:\n",
    "                if self.bbox_embed_diff_each_layer:\n",
    "                    tmp = self.bbox_embed[layer_id](output)\n",
    "                else:\n",
    "                    tmp = self.bbox_embed(output)\n",
    "                # import ipdb; ipdb.set_trace()\n",
    "                tmp[..., :self.query_dim] += inverse_sigmoid(reference_points)\n",
    "                new_reference_points = tmp[..., :self.query_dim].sigmoid()\n",
    "                if layer_id != self.num_layers - 1:\n",
    "                    ref_points.append(new_reference_points)\n",
    "                reference_points = new_reference_points.detach()\n",
    "\n",
    "            if self.return_intermediate:\n",
    "                intermediate.append(self.norm(output))\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "            if self.return_intermediate:\n",
    "                intermediate.pop()\n",
    "                intermediate.append(output)\n",
    "\n",
    "        if self.return_intermediate:\n",
    "            if self.bbox_embed is not None:\n",
    "                return [\n",
    "                    torch.stack(intermediate).transpose(1, 2),\n",
    "                    torch.stack(ref_points).transpose(1, 2),\n",
    "                ]\n",
    "            else:\n",
    "                return [\n",
    "                    torch.stack(intermediate).transpose(1, 2), \n",
    "                    reference_points.unsqueeze(0).transpose(1, 2)\n",
    "                ]\n",
    "\n",
    "        return output.unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
    "                 activation=\"relu\", normalize_before=False, keep_query_pos=False,\n",
    "                 rm_self_attn_decoder=False):\n",
    "        super().__init__()\n",
    "        # Decoder Self-Attention\n",
    "        if not rm_self_attn_decoder:\n",
    "            self.sa_qcontent_proj = nn.Linear(d_model, d_model)\n",
    "            self.sa_qpos_proj = nn.Linear(d_model, d_model)\n",
    "            self.sa_kcontent_proj = nn.Linear(d_model, d_model)\n",
    "            self.sa_kpos_proj = nn.Linear(d_model, d_model)\n",
    "            self.sa_v_proj = nn.Linear(d_model, d_model)\n",
    "            self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, vdim=d_model)\n",
    "\n",
    "            self.norm1 = nn.LayerNorm(d_model)\n",
    "            self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        # Decoder Cross-Attention\n",
    "        self.ca_qcontent_proj = nn.Linear(d_model, d_model) # linear transformation to produce query content using y=Ax+b A= weight matrix, b = bias\n",
    "        self.ca_qpos_proj = nn.Linear(d_model, d_model)\n",
    "        self.ca_kcontent_proj = nn.Linear(d_model, d_model)\n",
    "        self.ca_kpos_proj = nn.Linear(d_model, d_model)\n",
    "        self.ca_v_proj = nn.Linear(d_model, d_model)\n",
    "        self.ca_qpos_sine_proj = nn.Linear(d_model, d_model)\n",
    "        self.cross_attn = MultiheadAttention(d_model*2, nhead, dropout=dropout, vdim=d_model)\n",
    "\n",
    "        self.nhead = nhead\n",
    "        self.rm_self_attn_decoder = rm_self_attn_decoder\n",
    "\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        \n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "        self.normalize_before = normalize_before\n",
    "        self.keep_query_pos = keep_query_pos\n",
    "\n",
    "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
    "        return tensor if pos is None else tensor + pos\n",
    "\n",
    "    def forward(self, tgt, memory,\n",
    "                     tgt_mask: Optional[Tensor] = None,\n",
    "                     memory_mask: Optional[Tensor] = None,\n",
    "                     tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                     memory_key_padding_mask: Optional[Tensor] = None,\n",
    "                     pos: Optional[Tensor] = None,\n",
    "                     query_pos: Optional[Tensor] = None,\n",
    "                     query_sine_embed = None,\n",
    "                     is_first = False):\n",
    "                     \n",
    "        # ========== Begin of Self-Attention =============\n",
    "        if not self.rm_self_attn_decoder:\n",
    "            # Apply projections here\n",
    "            # shape: num_queries x batch_size x 256\n",
    "            q_content = self.sa_qcontent_proj(tgt)      # target is the input of the first decoder layer. zero by default.\n",
    "            q_pos = self.sa_qpos_proj(query_pos)\n",
    "            k_content = self.sa_kcontent_proj(tgt)\n",
    "            k_pos = self.sa_kpos_proj(query_pos)\n",
    "            v = self.sa_v_proj(tgt)\n",
    "\n",
    "            num_queries, bs, n_model = q_content.shape\n",
    "            hw, _, _ = k_content.shape\n",
    "\n",
    "            q = q_content + q_pos\n",
    "            k = k_content + k_pos\n",
    "\n",
    "            tgt2 = self.self_attn(q, k, value=v, attn_mask=tgt_mask,\n",
    "                                key_padding_mask=tgt_key_padding_mask)[0]\n",
    "            # ========== End of Self-Attention =============\n",
    "\n",
    "            tgt = tgt + self.dropout1(tgt2)\n",
    "            tgt = self.norm1(tgt)\n",
    "\n",
    "        # ========== Begin of Cross-Attention =============\n",
    "        # Apply projections here\n",
    "        # shape: num_queries x batch_size x 256\n",
    "        q_content = self.ca_qcontent_proj(tgt)\n",
    "        k_content = self.ca_kcontent_proj(memory)\n",
    "        v = self.ca_v_proj(memory)\n",
    "\n",
    "        num_queries, bs, n_model = q_content.shape\n",
    "        hw, _, _ = k_content.shape\n",
    "\n",
    "        k_pos = self.ca_kpos_proj(pos)\n",
    "\n",
    "        # For the first decoder layer, we concatenate the positional embedding predicted from \n",
    "        # the object query (the positional embedding) into the original query (key) in DETR.\n",
    "        if is_first or self.keep_query_pos:\n",
    "            q_pos = self.ca_qpos_proj(query_pos)\n",
    "            q = q_content + q_pos\n",
    "            k = k_content + k_pos\n",
    "        else:\n",
    "            q = q_content\n",
    "            k = k_content\n",
    "\n",
    "        q = q.view(num_queries, bs, self.nhead, n_model//self.nhead)\n",
    "        query_sine_embed = self.ca_qpos_sine_proj(query_sine_embed)\n",
    "        query_sine_embed = query_sine_embed.view(num_queries, bs, self.nhead, n_model//self.nhead)\n",
    "        q = torch.cat([q, query_sine_embed], dim=3).view(num_queries, bs, n_model * 2)\n",
    "        k = k.view(hw, bs, self.nhead, n_model//self.nhead)\n",
    "        k_pos = k_pos.view(hw, bs, self.nhead, n_model//self.nhead)\n",
    "        k = torch.cat([k, k_pos], dim=3).view(hw, bs, n_model * 2)\n",
    "\n",
    "        tgt2 = self.cross_attn(query=q,\n",
    "                                   key=k,\n",
    "                                   value=v, attn_mask=memory_mask,\n",
    "                                   key_padding_mask=memory_key_padding_mask)[0]               \n",
    "        # ========== End of Cross-Attention =============\n",
    "\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt   # tgt is the output of the first decoder layer. zero by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ProjectionLayer needs to be edited to included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class ProjectionLayer(nn.Module):\n",
    "\n",
    "    #def __init__(self, d_model, vocab_size) -> None:\n",
    "        #super().__init__()\n",
    "        #self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    #def forward(self, x) -> None:\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
    "        #return self.proj(x)\n",
    "    \n",
    "class ProjectionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, num_classes, box_dim) -> None: # d_model = 256, num_classes = 91, box_dim = 4\n",
    "        super().__init__()\n",
    "        self.class_proj = nn.Linear(d_model, num_classes)\n",
    "        self.box_proj = nn.Linear(d_model, box_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, num_classes)\n",
    "        class_logits = self.class_proj(x)\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, box_dim)\n",
    "        box_regression = self.box_proj(x)\n",
    "        return class_logits, box_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model=256, nhead=8, num_queries=300, num_encoder_layers=6,\n",
    "                 num_decoder_layers=6, dim_feedforward=2048, dropout=0.1,\n",
    "                 activation=\"relu\", normalize_before=False,\n",
    "                 return_intermediate_dec=False, query_dim=4,\n",
    "                 keep_query_pos=False, query_scale_type='cond_elewise',\n",
    "                 num_patterns=0,\n",
    "                 modulate_hw_attn=True,\n",
    "                 bbox_embed_diff_each_layer=False,\n",
    "                 ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward,\n",
    "                                                dropout, activation, normalize_before)\n",
    "        encoder_norm = nn.LayerNorm(d_model) if normalize_before else None\n",
    "        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n",
    "\n",
    "        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward,\n",
    "                                                dropout, activation, normalize_before, keep_query_pos=keep_query_pos)\n",
    "        decoder_norm = nn.LayerNorm(d_model)\n",
    "        self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm,\n",
    "                                          return_intermediate=return_intermediate_dec,\n",
    "                                          d_model=d_model, query_dim=query_dim, keep_query_pos=keep_query_pos, query_scale_type=query_scale_type,\n",
    "                                          modulate_hw_attn=modulate_hw_attn,\n",
    "                                          bbox_embed_diff_each_layer=bbox_embed_diff_each_layer)\n",
    "\n",
    "        self._reset_parameters()\n",
    "        assert query_scale_type in ['cond_elewise', 'cond_scalar', 'fix_elewise']\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.dec_layers = num_decoder_layers\n",
    "        self.num_queries = num_queries\n",
    "        self.num_patterns = num_patterns\n",
    "        if not isinstance(num_patterns, int):\n",
    "            Warning(\"num_patterns should be int but {}\".format(type(num_patterns)))\n",
    "            self.num_patterns = 0\n",
    "        if self.num_patterns > 0:\n",
    "            self.patterns = nn.Embedding(self.num_patterns, d_model)\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, src, mask, refpoint_embed, pos_embed):\n",
    "        # flatten NxCxHxW to HWxNxC\n",
    "        bs, c, h, w = src.shape\n",
    "        src = src.flatten(2).permute(2, 0, 1)\n",
    "        pos_embed = pos_embed.flatten(2).permute(2, 0, 1)\n",
    "        refpoint_embed = refpoint_embed.unsqueeze(1).repeat(1, bs, 1)\n",
    "        mask = mask.flatten(1)        \n",
    "        memory = self.encoder(src, src_key_padding_mask=mask, pos=pos_embed)\n",
    "\n",
    "        # query_embed = gen_sineembed_for_position(refpoint_embed)\n",
    "        num_queries = refpoint_embed.shape[0]\n",
    "        if self.num_patterns == 0:\n",
    "            tgt = torch.zeros(num_queries, bs, self.d_model, device=refpoint_embed.device)\n",
    "        else:\n",
    "            tgt = self.patterns.weight[:, None, None, :].repeat(1, self.num_queries, bs, 1).flatten(0, 1) # n_q*n_pat, bs, d_model\n",
    "            refpoint_embed = refpoint_embed.repeat(self.num_patterns, 1, 1) # n_q*n_pat, bs, d_model\n",
    "            # import ipdb; ipdb.set_trace()\n",
    "        hs, references = self.decoder(tgt, memory, memory_key_padding_mask=mask,\n",
    "                          pos=pos_embed, refpoints_unsigmoid=refpoint_embed)\n",
    "        print(hs)\n",
    "        print(references)\n",
    "        return hs, references    # Transformer class, the return value will be a tuple containing hs and references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(hidden_dim, dropout, nheads, num_queries, dim_feedforward, enc_layers, dec_layers,  num_patterns, pre_norm = False,\n",
    "                      transformer_activation = 'prelu',\n",
    "                      ):\n",
    "    return Transformer(\n",
    "        d_model=hidden_dim,\n",
    "        dropout=dropout,\n",
    "        nhead=nheads,\n",
    "        num_queries=num_queries,\n",
    "        dim_feedforward=dim_feedforward,\n",
    "        num_encoder_layers=enc_layers,\n",
    "        num_decoder_layers=dec_layers,\n",
    "        normalize_before=pre_norm,\n",
    "        return_intermediate_dec=True,\n",
    "        query_dim=4,\n",
    "        activation=transformer_activation,\n",
    "        num_patterns=num_patterns,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_tensor(tensor, title):\n",
    "    tensor = tensor.detach().cpu().numpy()  # Detach from the computation graph and convert to numpy\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(tensor, cmap='gray')  # Display the tensor as an image\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X_DETR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_focal_loss(inputs, targets, num_boxes, alpha: float = 0.25, gamma: float = 2):\n",
    "    prob = inputs.sigmoid()\n",
    "    ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n",
    "    p_t = prob * targets + (1 - prob) * (1 - targets)\n",
    "    loss = ce_loss * ((1 - p_t) ** gamma)\n",
    "\n",
    "    if alpha >= 0:\n",
    "        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n",
    "        loss = alpha_t * loss\n",
    "\n",
    "    target_loss = loss.mean(1).sum() / num_boxes\n",
    "\n",
    "    return target_loss\n",
    "class DABDETR (nn.Module):\n",
    "    def __init__(self, resnetbackbone, transformer, num_classes, num_queries, dec_layers, iter_update, query_dim, aux_loss=False, \n",
    "               bbox_embed_diff_each_layer=False, random_refpoints_xy=False):\n",
    "        super().__init__()\n",
    "        self.num_queries = num_queries\n",
    "        self.transformer = transformer\n",
    "        hidden_dim = transformer.d_model\n",
    "        self.class_embed = nn.Linear(hidden_dim, num_classes)\n",
    "        self.bbox_embed_diff_each_layer = bbox_embed_diff_each_layer\n",
    "        if bbox_embed_diff_each_layer:\n",
    "            self.bbox_embed = nn.ModuleList([MLP(hidden_dim, hidden_dim, 4, 3) for i in range(dec_layers)]) #  a different embedding is created for each decoder layer. This is done using PyTorch's nn.ModuleList,\n",
    "        else:\n",
    "            self.bbox_embed = MLP(hidden_dim, hidden_dim, 4, 3)\n",
    "        \n",
    "        # setting query dim\n",
    "        self.query_dim = query_dim\n",
    "        assert query_dim in [2, 4]\n",
    "\n",
    "        self.refpoint_embed = nn.Embedding(num_queries, query_dim)\n",
    "        self.random_refpoints_xy = random_refpoints_xy\n",
    "        if random_refpoints_xy:\n",
    "            # import ipdb; ipdb.set_trace()\n",
    "            self.refpoint_embed.weight.data[:, :2].uniform_(0,1)\n",
    "            self.refpoint_embed.weight.data[:, :2] = inverse_sigmoid(self.refpoint_embed.weight.data[:, :2])\n",
    "            self.refpoint_embed.weight.data[:, :2].requires_grad = False\n",
    "\n",
    "        self.input_proj = nn.Conv2d(resnetbackbone.num_channels[-1], hidden_dim, kernel_size=1)\n",
    "        self.resnetbackbone = resnetbackbone\n",
    "        self.aux_loss = aux_loss\n",
    "        self.iter_update = iter_update\n",
    "\n",
    "        if self.iter_update:\n",
    "            self.transformer.decoder.bbox_embed = self.bbox_embed\n",
    "\n",
    "        # init prior_prob setting for focal loss\n",
    "        prior_prob = 0.01\n",
    "        bias_value = -math.log((1 - prior_prob) / prior_prob)\n",
    "        self.class_embed.bias.data = torch.ones(num_classes) * bias_value\n",
    "\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        # init bbox_embed\n",
    "        if bbox_embed_diff_each_layer:\n",
    "            for bbox_embed in self.bbox_embed:\n",
    "                nn.init.constant_(bbox_embed.layers[-1].weight.data, 0)\n",
    "                nn.init.constant_(bbox_embed.layers[-1].bias.data, 0)\n",
    "        else:\n",
    "            nn.init.constant_(self.bbox_embed.layers[-1].weight.data, 0)\n",
    "            nn.init.constant_(self.bbox_embed.layers[-1].bias.data, 0)\n",
    "    \n",
    "    def forward(self,  samples: NestedTensor):\n",
    "        if isinstance(samples, (list, torch.Tensor)):\n",
    "            sample = nested_tensor_from_tensor_list(sample, sample.tensors)\n",
    "        features, pos = self.resnetbackbone(samples)\n",
    "\n",
    "        src, mask = features[-1].decompose()\n",
    "        assert mask is not None\n",
    "        # default pipeline\n",
    "        embedweight = self.refpoint_embed.weight\n",
    "        \n",
    "        \n",
    "        hs, reference = self.transformer(self.input_proj(src), mask, embedweight, pos[-1])  #Transformer class, the return value will be a tuple containing hs and references\n",
    "        print(hs, reference)\n",
    "        # Visualize the first slice of the hs tensor\n",
    "        visualize_tensor(hs[0], 'hs')\n",
    "\n",
    "        # Visualize the first slice of the references tensor\n",
    "        visualize_tensor(references[0], 'references')\n",
    "        \n",
    "        \n",
    "        if not self.bbox_embed_diff_each_layer:\n",
    "            reference_before_sigmoid = inverse_sigmoid(reference)\n",
    "            tmp = self.bbox_embed(hs)\n",
    "            tmp[..., :self.query_dim] += reference_before_sigmoid\n",
    "            outputs_coord = tmp.sigmoid()\n",
    "        else:\n",
    "            reference_before_sigmoid = inverse_sigmoid(reference)\n",
    "            outputs_coords = []\n",
    "            for lvl in range(hs.shape[0]):\n",
    "                tmp = self.bbox_embed[lvl](hs[lvl])\n",
    "                tmp[..., :self.query_dim] += reference_before_sigmoid[lvl]\n",
    "                outputs_coord = tmp.sigmoid()\n",
    "                outputs_coords.append(outputs_coord)\n",
    "            outputs_coord = torch.stack(outputs_coords)\n",
    "\n",
    "        outputs_class = self.class_embed(hs)\n",
    "        out = {'pred_logits': outputs_class[-1], 'pred_boxes': outputs_coord[-1]}\n",
    "        if self.aux_loss:\n",
    "            out['aux_outputs'] = self._set_aux_loss(outputs_class, outputs_coord)\n",
    "        return out\n",
    "\n",
    "    @torch.jit.unused\n",
    "    def _set_aux_loss(self, outputs_class, outputs_coord):\n",
    "        # this is a workaround to make torchscript happy, as torchscript\n",
    "        # doesn't support dictionary with non-homogeneous values, such\n",
    "        # as a dict having both a Tensor and a list.\n",
    "        return [{'pred_logits': a, 'pred_boxes': b}\n",
    "                for a, b in zip(outputs_class[:-1], outputs_coord[:-1])]\n",
    "        \n",
    "class SetCriterion(nn.Module):\n",
    "    def __init__(self, num_classes, matcher, weight_dict, losses, focal_alpha):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.matcher = matcher\n",
    "        self.weight_dict = weight_dict\n",
    "        self.losses = losses\n",
    "        self.focal_alpha = focal_alpha\n",
    "        \n",
    "    def loss_lebel(self, outputs, targets, indices, num_boxes, log=True):\n",
    "        assert 'pred_logits' in outputs\n",
    "        src_logits = outputs['pred_logits']\n",
    "\n",
    "        idx = self._get_src_permutation_idx(indices)\n",
    "        target_classes_o = torch.cat([t[\"labels\"][J] for t, (_, J) in zip(targets, indices)])\n",
    "        target_classes = torch.full(src_logits.shape[:2], self.num_classes,\n",
    "                                    dtype=torch.int64, device=src_logits.device)\n",
    "        target_classes[idx] = target_classes_o\n",
    "\n",
    "        target_classes_onehot = torch.zeros([src_logits.shape[0], src_logits.shape[1], src_logits.shape[2]+1],\n",
    "                                            dtype=src_logits.dtype, layout=src_logits.layout, device=src_logits.device)\n",
    "        target_classes_onehot.scatter_(2, target_classes.unsqueeze(-1), 1)\n",
    "\n",
    "        target_classes_onehot = target_classes_onehot[:,:,:-1]\n",
    "        loss_ce = sigmoid_focal_loss(src_logits, target_classes_onehot, num_boxes, alpha=self.focal_alpha, gamma=2) * src_logits.shape[1]\n",
    "        losses = {'loss_ce': loss_ce}\n",
    "\n",
    "        if log:\n",
    "            # TODO this should probably be a separate loss, not hacked in this one here\n",
    "            losses['class_error'] = 100 - accuracy(src_logits[idx], target_classes_o)[0]\n",
    "        return losses \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def loss_cardinality(self, outputs, targets, indices, num_boxes): # num_boxes the number of bounding boxes in each image\n",
    "        pred_logits = outputs['pred_logits']\n",
    "        device = pred_logits.device\n",
    "        tgt_lengths = torch.as_tensor([len(v[\"labels\"]) for v in targets], device=device)\n",
    "        # Count the number of predictions that are NOT \"no-object\" (which is the last class)\n",
    "        card_pred = (pred_logits.argmax(-1) != pred_logits.shape[-1] - 1).sum(1)\n",
    "        card_err = F.l1_loss(card_pred.float(), tgt_lengths.float())\n",
    "        losses = {'cardinality_error': card_err}\n",
    "        return losses\n",
    "\n",
    "    def loss_boxes(self, outputs, targets, indices, num_boxes):\n",
    "        assert 'pred_boxes' in outputs\n",
    "        idx = self._get_src_permutation_idx(indices)\n",
    "        src_boxes = outputs['pred_boxes'][idx]\n",
    "        target_boxes = torch.cat([t['boxes'][i] for t, (_, i) in zip(targets, indices)], dim=0)\n",
    "\n",
    "        loss_bbox = F.l1_loss(src_boxes, target_boxes, reduction='none')\n",
    "\n",
    "        losses = {}\n",
    "        losses['loss_bbox'] = loss_bbox.sum() / num_boxes\n",
    "\n",
    "        loss_giou = 1 - torch.diag(box_ops.generalized_box_iou(\n",
    "            box_ops.box_cxcywh_to_xyxy(src_boxes),\n",
    "            box_ops.box_cxcywh_to_xyxy(target_boxes)))\n",
    "        losses['loss_giou'] = loss_giou.sum() / num_boxes\n",
    "\n",
    "        # calculate the x,y and h,w loss\n",
    "        with torch.no_grad():\n",
    "            losses['loss_xy'] = loss_bbox[..., :2].sum() / num_boxes\n",
    "            losses['loss_hw'] = loss_bbox[..., 2:].sum() / num_boxes\n",
    "\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def loss_masks(self, outputs, targets, indices, num_boxes):\n",
    "        \"\"\"Compute the losses related to the masks: the focal loss and the dice loss.\n",
    "        Dice Loss is a popular loss function used for image segmentation tasks, particularly for medical image segmentation.\n",
    "           targets dicts must contain the key \"masks\" containing a tensor of dim [nb_target_boxes, h, w]\n",
    "        \"\"\"\n",
    "        assert \"pred_masks\" in outputs\n",
    "\n",
    "        src_idx = self._get_src_permutation_idx(indices)\n",
    "        tgt_idx = self._get_tgt_permutation_idx(indices)\n",
    "        src_masks = outputs[\"pred_masks\"]\n",
    "        src_masks = src_masks[src_idx]\n",
    "        masks = [t[\"masks\"] for t in targets]\n",
    "        # TODO use valid to mask invalid areas due to padding in loss\n",
    "        target_masks, valid = nested_tensor_from_tensor_list(masks).decompose()\n",
    "        target_masks = target_masks.to(src_masks)\n",
    "        target_masks = target_masks[tgt_idx]\n",
    "\n",
    "        # upsample predictions to the target size\n",
    "        src_masks = interpolate(src_masks[:, None], size=target_masks.shape[-2:],\n",
    "                                mode=\"bilinear\", align_corners=False)\n",
    "        src_masks = src_masks[:, 0].flatten(1)\n",
    "\n",
    "        target_masks = target_masks.flatten(1)\n",
    "        target_masks = target_masks.view(src_masks.shape)\n",
    "        losses = {\n",
    "            \"loss_mask\": sigmoid_focal_loss(src_masks, target_masks, num_boxes),\n",
    "            #\"loss_dice\": dice_loss(src_masks, target_masks, num_boxes),\n",
    "            # Dice Loss is a loss function used for image segmentation tasks, particularly for medical image segmentation. \n",
    "        }\n",
    "        return losses    \n",
    "    \n",
    "    def _get_src_permutation_idx(self, indices):\n",
    "        # permute predictions following indices\n",
    "        batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n",
    "        src_idx = torch.cat([src for (src, _) in indices])\n",
    "        return batch_idx, src_idx\n",
    "\n",
    "    def _get_tgt_permutation_idx(self, indices):\n",
    "        # permute targets following indices\n",
    "        batch_idx = torch.cat([torch.full_like(tgt, i) for i, (_, tgt) in enumerate(indices)])\n",
    "        tgt_idx = torch.cat([tgt for (_, tgt) in indices])\n",
    "        return batch_idx, tgt_idx\n",
    "\n",
    "    def get_loss(self, loss, outputs, targets, indices, num_boxes, **kwargs):\n",
    "        loss_map = {\n",
    "            'labels': self.loss_labels,\n",
    "            'cardinality': self.loss_cardinality,\n",
    "            'boxes': self.loss_boxes,\n",
    "            'masks': self.loss_masks\n",
    "        }\n",
    "        assert loss in loss_map, f'do you really want to compute {loss} loss?'\n",
    "        return loss_map[loss](outputs, targets, indices, num_boxes, **kwargs)\n",
    "\n",
    "    def forward(self, outputs, targets, return_indices=False):\n",
    "\n",
    "        outputs_without_aux = {k: v for k, v in outputs.items() if k != 'aux_outputs'}\n",
    "\n",
    "        # Retrieve the matching between the outputs of the last layer and the targets\n",
    "        indices = self.matcher(outputs_without_aux, targets)\n",
    "        if return_indices:\n",
    "            indices0_copy = indices\n",
    "            indices_list = []\n",
    "\n",
    "        # Compute the average number of target boxes accross all nodes, for normalization purposes\n",
    "        num_boxes = sum(len(t[\"labels\"]) for t in targets)\n",
    "        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)\n",
    "        if is_dist_avail_and_initialized():\n",
    "            torch.distributed.all_reduce(num_boxes)\n",
    "        num_boxes = torch.clamp(num_boxes / get_world_size(), min=1).item()\n",
    "\n",
    "        # Compute all the requested losses\n",
    "        losses = {}\n",
    "        for loss in self.losses:\n",
    "            losses.update(self.get_loss(loss, outputs, targets, indices, num_boxes))\n",
    "\n",
    "        # In case of auxiliary losses, we repeat this process with the output of each intermediate layer.\n",
    "        if 'aux_outputs' in outputs:\n",
    "            for i, aux_outputs in enumerate(outputs['aux_outputs']): # i index \n",
    "                indices = self.matcher(aux_outputs, targets)\n",
    "                if return_indices:\n",
    "                    indices_list.append(indices)\n",
    "                for loss in self.losses:\n",
    "                    if loss == 'masks':\n",
    "                        # Intermediate masks losses are too costly to compute, we ignore them.\n",
    "                        continue\n",
    "                    kwargs = {}\n",
    "                    if loss == 'labels':\n",
    "                        # Logging is enabled only for the last layer\n",
    "                        kwargs = {'log': False}\n",
    "                    l_dict = self.get_loss(loss, aux_outputs, targets, indices, num_boxes, **kwargs)\n",
    "                    l_dict = {k + f'_{i}': v for k, v in l_dict.items()}\n",
    "                    losses.update(l_dict)\n",
    "\n",
    "        if return_indices:\n",
    "            indices_list.append(indices0_copy)\n",
    "            return losses, indices_list\n",
    "\n",
    "        return losses\n",
    "\n",
    "class PostProcess(nn.Module):\n",
    "    \"\"\" This module converts the model's output into the format expected by the coco api\"\"\"\n",
    "    def __init__(self, num_select=100) -> None:\n",
    "        super().__init__()\n",
    "        self.num_select = num_select\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, outputs, target_sizes):\n",
    "        num_select = self.num_select\n",
    "        out_logits, out_bbox = outputs['pred_logits'], outputs['pred_boxes']\n",
    "\n",
    "        assert len(out_logits) == len(target_sizes)\n",
    "        assert target_sizes.shape[1] == 2\n",
    "\n",
    "        prob = out_logits.sigmoid()\n",
    "        topk_values, topk_indexes = torch.topk(prob.view(out_logits.shape[0], -1), num_select, dim=1)\n",
    "        scores = topk_values\n",
    "        topk_boxes = topk_indexes // out_logits.shape[2]\n",
    "        labels = topk_indexes % out_logits.shape[2]\n",
    "        boxes = box_ops.box_cxcywh_to_xyxy(out_bbox)\n",
    "        boxes = torch.gather(boxes, 1, topk_boxes.unsqueeze(-1).repeat(1,1,4))\n",
    "        \n",
    "        # and from relative [0, 1] to absolute [0, height] coordinates\n",
    "        img_h, img_w = target_sizes.unbind(1)\n",
    "        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)\n",
    "        boxes = boxes * scale_fct[:, None, :]\n",
    "\n",
    "        results = [{'scores': s, 'labels': l, 'boxes': b} for s, l, b in zip(scores, labels, boxes)]\n",
    "\n",
    "        return results\n",
    "\n",
    "def build_DABDETR(dataset_file, device, cls_loss_coef, bbox_loss_coef, giou_loss_coef, mask_loss_coef,dice_loss_coef, lr_backbone , dropout, position_embedding, transformer_activation, \n",
    "                  num_patterns, random_refpoints_xy, hidden_dim, nheads, num_queries, backbone, num_feature_levels, dim_feedforward, enc_layers, dec_layers, \n",
    "                  focal_alpha, num_select, iter_update, query_dim, bbox_embed_diff_each_layer, pre_norm = False, aux_loss = True, masks= False,):\n",
    "    num_classes = 20 if dataset_file != 'coco' else 91\n",
    "    if dataset_file == \"coco_panoptic\":\n",
    "        num_classes = 250\n",
    "    device = torch.device(device)\n",
    "\n",
    "    backbone = build_backbone( position_embedding, lr_backbone ,masks, num_feature_levels, backbone, dilation = True)  # arg changed\n",
    "    \n",
    "\n",
    "    transformer = build_transformer(  hidden_dim, dropout, nheads, num_queries, dim_feedforward, enc_layers, dec_layers, num_patterns ,\n",
    "                                      pre_norm, transformer_activation )  # arg changed\n",
    "    \n",
    "#self, resnetbackbone, transformer, num_classes, num_queries, num_dec_layers, iter_update, query_dim, aux_loss=False, \n",
    "               #bbox_embed_diff_each_layer=False, random_refpoints_xy=False\n",
    "    model = DABDETR(\n",
    "        backbone,\n",
    "        transformer,\n",
    "        num_classes,\n",
    "        num_queries,\n",
    "        dec_layers,\n",
    "        iter_update,\n",
    "        query_dim,\n",
    "        aux_loss,\n",
    "        bbox_embed_diff_each_layer,\n",
    "        random_refpoints_xy,\n",
    "    ) # arg changed\n",
    "    \n",
    "    # resnetbackbone, transformer, num_classes, num_queries, num_dec_layers, iter_update, query_dim,\n",
    "    \n",
    "    \"\"\" the code that are made for the segmentation task are commented out\"\"\"\n",
    "    \n",
    "    #if args.masks:\n",
    "        #model = DETRsegm(model, freeze_detr=(args.frozen_weights is not None))\n",
    "        \n",
    "    matcher = matcherr.build_matcher( set_cost_class = 2, set_cost_bbox = 5, set_cost_giou = 2, focal_alpha=0.25 ) # arg \n",
    "    weight_dict = {'loss_ce': cls_loss_coef, 'loss_bbox': bbox_loss_coef}\n",
    "    weight_dict['loss_giou'] = giou_loss_coef\n",
    "    if masks:\n",
    "        weight_dict[\"loss_mask\"] = mask_loss_coef\n",
    "        weight_dict[\"loss_dice\"] = dice_loss_coef\n",
    "    # TODO this is a hack\n",
    "    if aux_loss:\n",
    "        aux_weight_dict = {}\n",
    "        for i in range(dec_layers - 1):\n",
    "            aux_weight_dict.update({k + f'_{i}': v for k, v in weight_dict.items()})\n",
    "        weight_dict.update(aux_weight_dict)\n",
    "\n",
    "    losses = ['labels', 'boxes', 'cardinality']\n",
    "    if masks:\n",
    "        losses += [\"masks\"]\n",
    "        #num_classes, matcher, weight_dict, losses, focal_alpha\n",
    "    criterion = SetCriterion (num_classes, matcher, weight_dict, losses, focal_alpha) # arg \n",
    "    criterion.to(device)\n",
    "    postprocessors = {'bbox': PostProcess(num_select)} # arg \n",
    "    #if args.masks:\n",
    "        #postprocessors['segm'] = PostProcessSegm()\n",
    "        #if args.dataset_file == \"coco_panoptic\":\n",
    "            #is_thing_map = {i: i <= 90 for i in range(201)}\n",
    "            #postprocessors[\"panoptic\"] = PostProcessPanoptic(is_thing_map, threshold=0.85)\n",
    "\n",
    "    return model, criterion, postprocessors\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engine For Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
    "\"\"\"\n",
    "Train and eval functions used in main.py\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "from typing import Iterable\n",
    "\n",
    "from util.utils import slprint, to_device\n",
    "\n",
    "import torch\n",
    "\n",
    "import util.misc as utils\n",
    "\n",
    "\n",
    "def train_one_epoch(use_dn , model: torch.nn.Module, criterion: torch.nn.Module,\n",
    "                    data_loader: Iterable, optimizer: torch.optim.Optimizer,\n",
    "                    device: torch.device, epoch: int, max_norm: float = 0, \n",
    "                    wo_class_error=False, lr_scheduler=None, amp = False, debug = False, logger=None, ema_m=None):\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
    "\n",
    "    try:\n",
    "        need_tgt_for_training = use_dn\n",
    "    except:\n",
    "        need_tgt_for_training = False\n",
    "\n",
    "    model.train()\n",
    "    criterion.train()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    if not wo_class_error:\n",
    "        metric_logger.add_meter('class_error', utils.SmoothedValue(window_size=1, fmt='{value:.2f}'))\n",
    "    header = 'Epoch: [{}]'.format(epoch)\n",
    "    print_freq = 10\n",
    "\n",
    "    _cnt = 0\n",
    "    for samples, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
    "\n",
    "        samples = samples.to(device)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=amp):\n",
    "            if need_tgt_for_training:\n",
    "                outputs = model(samples, targets)\n",
    "            else:\n",
    "                outputs = model(samples)\n",
    "        \n",
    "            loss_dict = criterion(outputs, targets)\n",
    "            weight_dict = criterion.weight_dict\n",
    "            losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    "\n",
    "\n",
    "        # reduce losses over all GPUs for logging purposes\n",
    "        loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
    "        loss_dict_reduced_unscaled = {f'{k}_unscaled': v\n",
    "                                      for k, v in loss_dict_reduced.items()}\n",
    "        loss_dict_reduced_scaled = {k: v * weight_dict[k]\n",
    "                                    for k, v in loss_dict_reduced.items() if k in weight_dict}\n",
    "        losses_reduced_scaled = sum(loss_dict_reduced_scaled.values())\n",
    "\n",
    "        loss_value = losses_reduced_scaled.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "            print(loss_dict_reduced)\n",
    "            sys.exit(1)\n",
    "\n",
    "\n",
    "        # amp backward function\n",
    "        if amp:\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(losses).backward()\n",
    "            if max_norm > 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            # original backward function\n",
    "            losses.backward()\n",
    "            if max_norm > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        metric_logger.update(loss=loss_value, **loss_dict_reduced_scaled, **loss_dict_reduced_unscaled)\n",
    "        if 'class_error' in loss_dict_reduced:\n",
    "            metric_logger.update(class_error=loss_dict_reduced['class_error'])\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "        _cnt += 1\n",
    "        if debug:\n",
    "            if _cnt % 15 == 0:\n",
    "                print(\"BREAK!\"*5)\n",
    "                break\n",
    "\n",
    "\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "    resstat = {k: meter.global_avg for k, meter in metric_logger.meters.items() if meter.count > 0}\n",
    "    if getattr(criterion, 'loss_weight_decay', False):\n",
    "        resstat.update({f'weight_{k}': v for k,v in criterion.weight_dict.items()})\n",
    "    return resstat\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, criterion, postprocessors, data_loader, amp, use_dn, base_ds, device, output_dir = '', debug= False, \n",
    "             save_results = False, wo_class_error=False, logger=None):\n",
    "    try:\n",
    "        need_tgt_for_training = use_dn\n",
    "    except:\n",
    "        need_tgt_for_training = False\n",
    "\n",
    "    model.eval()\n",
    "    criterion.eval()\n",
    "\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    if not wo_class_error:\n",
    "        metric_logger.add_meter('class_error', utils.SmoothedValue(window_size=1, fmt='{value:.2f}'))\n",
    "    header = 'Test:'\n",
    "\n",
    "    iou_types = tuple(k for k in ('segm', 'bbox') if k in postprocessors.keys())\n",
    "    coco_evaluator = CocoEvaluator(base_ds, iou_types)\n",
    "\n",
    "    \"\"\"panoptic_evaluator = None\n",
    "    if 'panoptic' in postprocessors.keys():\n",
    "        panoptic_evaluator = PanopticEvaluator(\n",
    "            data_loader.dataset.ann_file,\n",
    "            data_loader.dataset.ann_folder,\n",
    "            output_dir=os.path.join(output_dir, \"panoptic_eval\"),\n",
    "        )\"\"\"\n",
    "\n",
    "    _cnt = 0\n",
    "    output_state_dict = {} # for debug only\n",
    "    for samples, targets in metric_logger.log_every(data_loader, 10, header):\n",
    "        samples = samples.to(device)\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        # targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        targets = [{k: to_device(v, device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=amp):\n",
    "            if need_tgt_for_training:\n",
    "                outputs = model(samples, targets)\n",
    "            else:\n",
    "                outputs = model(samples)\n",
    "            # outputs = model(samples)\n",
    "\n",
    "            loss_dict = criterion(outputs, targets)\n",
    "        weight_dict = criterion.weight_dict\n",
    "\n",
    "        # reduce losses over all GPUs for logging purposes\n",
    "        loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
    "        loss_dict_reduced_scaled = {k: v * weight_dict[k]\n",
    "                                    for k, v in loss_dict_reduced.items() if k in weight_dict}\n",
    "        loss_dict_reduced_unscaled = {f'{k}_unscaled': v\n",
    "                                      for k, v in loss_dict_reduced.items()}\n",
    "        metric_logger.update(loss=sum(loss_dict_reduced_scaled.values()),\n",
    "                             **loss_dict_reduced_scaled,\n",
    "                             **loss_dict_reduced_unscaled)\n",
    "        if 'class_error' in loss_dict_reduced:\n",
    "            metric_logger.update(class_error=loss_dict_reduced['class_error'])\n",
    "\n",
    "        orig_target_sizes = torch.stack([t[\"orig_size\"] for t in targets], dim=0)\n",
    "        results = postprocessors['bbox'](outputs, orig_target_sizes)\n",
    "        # [scores: [100], labels: [100], boxes: [100, 4]] x B\n",
    "        \n",
    "        \"\"\"if 'segm' in postprocessors.keys():\n",
    "            target_sizes = torch.stack([t[\"size\"] for t in targets], dim=0)\n",
    "            results = postprocessors['segm'](results, outputs, orig_target_sizes, target_sizes)\"\"\"\n",
    "            \n",
    "        res = {target['image_id'].item(): output for target, output in zip(targets, results)}\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        if coco_evaluator is not None:\n",
    "            coco_evaluator.update(res)\n",
    "\n",
    "        \"\"\"if panoptic_evaluator is not None:\n",
    "            res_pano = postprocessors[\"panoptic\"](outputs, target_sizes, orig_target_sizes)\n",
    "            for i, target in enumerate(targets):\n",
    "                image_id = target[\"image_id\"].item()\n",
    "                file_name = f\"{image_id:012d}.png\"\n",
    "                res_pano[i][\"image_id\"] = image_id\n",
    "                res_pano[i][\"file_name\"] = file_name\n",
    "\n",
    "            panoptic_evaluator.update(res_pano)\"\"\"\n",
    "        \n",
    "        if save_results:\n",
    "            \"\"\"\n",
    "            saving results of eval.\n",
    "            \"\"\"\n",
    "            # res_score = outputs['res_score']\n",
    "            # res_label = outputs['res_label']\n",
    "            # res_bbox = outputs['res_bbox']\n",
    "            # res_idx = outputs['res_idx']\n",
    "            # import ipdb; ipdb.set_trace()\n",
    "\n",
    "            for i, (tgt, res, outbbox) in enumerate(zip(targets, results, outputs['pred_boxes'])):\n",
    "                \"\"\"\n",
    "                pred vars:\n",
    "                    K: number of bbox pred\n",
    "                    score: Tensor(K),\n",
    "                    label: list(len: K),\n",
    "                    bbox: Tensor(K, 4)\n",
    "                    idx: list(len: K)\n",
    "                tgt: dict.\n",
    "\n",
    "                \"\"\"\n",
    "                # compare gt and res (after postprocess)\n",
    "                gt_bbox = tgt['boxes']\n",
    "                gt_label = tgt['labels']\n",
    "                gt_info = torch.cat((gt_bbox, gt_label.unsqueeze(-1)), 1)\n",
    "                \n",
    "                # img_h, img_w = tgt['orig_size'].unbind()\n",
    "                # scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=0)\n",
    "                # _res_bbox = res['boxes'] / scale_fct\n",
    "                _res_bbox = outbbox\n",
    "                _res_prob = res['scores']\n",
    "                _res_label = res['labels']\n",
    "                res_info = torch.cat((_res_bbox, _res_prob.unsqueeze(-1), _res_label.unsqueeze(-1)), 1)\n",
    "                # import ipdb;ipdb.set_trace()\n",
    "\n",
    "                if 'gt_info' not in output_state_dict:\n",
    "                    output_state_dict['gt_info'] = []\n",
    "                output_state_dict['gt_info'].append(gt_info.cpu())\n",
    "\n",
    "                if 'res_info' not in output_state_dict:\n",
    "                    output_state_dict['res_info'] = []\n",
    "                output_state_dict['res_info'].append(res_info.cpu())\n",
    "\n",
    "        _cnt += 1\n",
    "        if debug:\n",
    "            if _cnt % 15 == 0:\n",
    "                print(\"BREAK!\"*5)\n",
    "                break\n",
    "\n",
    "    if save_results:\n",
    "        import os.path as osp\n",
    "        savepath = osp.join(output_dir, 'results-{}.pkl'.format(utils.get_rank()))\n",
    "        print(\"Saving res to {}\".format(savepath))\n",
    "        torch.save(output_state_dict, savepath)\n",
    "\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "    if coco_evaluator is not None:\n",
    "        coco_evaluator.synchronize_between_processes()\n",
    "    #if panoptic_evaluator is not None:\n",
    "       # panoptic_evaluator.synchronize_between_processes()\n",
    "\n",
    "    # accumulate predictions from all images\n",
    "    if coco_evaluator is not None:\n",
    "        coco_evaluator.accumulate()\n",
    "        coco_evaluator.summarize()\n",
    "        \n",
    "    \"\"\"panoptic_res = None\n",
    "    if panoptic_evaluator is not None:\n",
    "        panoptic_res = panoptic_evaluator.summarize()\"\"\"\n",
    "        \n",
    "    stats = {k: meter.global_avg for k, meter in metric_logger.meters.items() if meter.count > 0}\n",
    "    if coco_evaluator is not None:\n",
    "        if 'bbox' in postprocessors.keys():\n",
    "            stats['coco_eval_bbox'] = coco_evaluator.coco_eval['bbox'].stats.tolist()\n",
    "        if 'segm' in postprocessors.keys():\n",
    "            stats['coco_eval_masks'] = coco_evaluator.coco_eval['segm'].stats.tolist()\n",
    "            \n",
    "    \"\"\"if panoptic_res is not None:\n",
    "        stats['PQ_all'] = panoptic_res[\"All\"]\n",
    "        stats['PQ_th'] = panoptic_res[\"Things\"]\n",
    "        stats['PQ_st'] = panoptic_res[\"Stuff\"]\"\"\"\n",
    "\n",
    "    # import ipdb; ipdb.set_trace()\n",
    "\n",
    "    return stats, coco_evaluator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/14 11:27:51.886]: git:\n",
      "  sha: N/A, status: clean, branch: N/A\n",
      "\n",
      "[03/14 11:27:51.887]: Command: C:\\Users\\Rahman\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel_launcher.py --f=c:\\Users\\Rahman\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-3080036yDElq5dKV9.json\n",
      "[03/14 11:27:51.888]: Full config saved to ./output_model\\config.json\n",
      "[03/14 11:27:51.888]: world size: 1\n",
      "[03/14 11:27:51.890]: rank: 0\n",
      "[03/14 11:27:51.890]: local_rank: 0\n",
      "[03/14 11:27:51.890]: output_dir: ./output_model\n",
      "[03/14 11:27:51.891]: frozen_weights: None\n",
      "[03/14 11:27:51.891]: masks: False\n",
      "output_dir: ./output_model\n",
      "rank: 0\n",
      "world_size: 1\n",
      "local_rank: 0\n",
      "frozen_weights: None\n",
      "masks: False\n",
      "<class 'float'> 1e-05\n",
      "embed_dim:256\n",
      "num_heads:8\n",
      "embed_dim:512\n",
      "num_heads:8\n",
      "[03/14 11:27:57.671]: number of params:43449629\n",
      "[03/14 11:27:57.673]: params:\n",
      "{\n",
      "  \"transformer.encoder.layers.0.self_attn.in_proj_weight\": 196608,\n",
      "  \"transformer.encoder.layers.0.self_attn.in_proj_bias\": 768,\n",
      "  \"transformer.encoder.layers.0.self_attn.out_proj.weight\": 65536,\n",
      "  \"transformer.encoder.layers.0.self_attn.out_proj.bias\": 256,\n",
      "  \"transformer.encoder.layers.0.linear1.weight\": 524288,\n",
      "  \"transformer.encoder.layers.0.linear1.bias\": 2048,\n",
      "  \"transformer.encoder.layers.0.linear2.weight\": 524288,\n",
      "  \"transformer.encoder.layers.0.linear2.bias\": 256,\n",
      "  \"transformer.encoder.layers.0.norm1.weight\": 256,\n",
      "  \"transformer.encoder.layers.0.norm1.bias\": 256,\n",
      "  \"transformer.encoder.layers.0.norm2.weight\": 256,\n",
      "  \"transformer.encoder.layers.0.norm2.bias\": 256,\n",
      "  \"transformer.encoder.layers.0.activation.weight\": 1,\n",
      "  \"transformer.encoder.layers.1.self_attn.in_proj_weight\": 196608,\n",
      "  \"transformer.encoder.layers.1.self_attn.in_proj_bias\": 768,\n",
      "  \"transformer.encoder.layers.1.self_attn.out_proj.weight\": 65536,\n",
      "  \"transformer.encoder.layers.1.self_attn.out_proj.bias\": 256,\n",
      "  \"transformer.encoder.layers.1.linear1.weight\": 524288,\n",
      "  \"transformer.encoder.layers.1.linear1.bias\": 2048,\n",
      "  \"transformer.encoder.layers.1.linear2.weight\": 524288,\n",
      "  \"transformer.encoder.layers.1.linear2.bias\": 256,\n",
      "  \"transformer.encoder.layers.1.norm1.weight\": 256,\n",
      "  \"transformer.encoder.layers.1.norm1.bias\": 256,\n",
      "  \"transformer.encoder.layers.1.norm2.weight\": 256,\n",
      "  \"transformer.encoder.layers.1.norm2.bias\": 256,\n",
      "  \"transformer.encoder.layers.1.activation.weight\": 1,\n",
      "  \"transformer.encoder.layers.2.self_attn.in_proj_weight\": 196608,\n",
      "  \"transformer.encoder.layers.2.self_attn.in_proj_bias\": 768,\n",
      "  \"transformer.encoder.layers.2.self_attn.out_proj.weight\": 65536,\n",
      "  \"transformer.encoder.layers.2.self_attn.out_proj.bias\": 256,\n",
      "  \"transformer.encoder.layers.2.linear1.weight\": 524288,\n",
      "  \"transformer.encoder.layers.2.linear1.bias\": 2048,\n",
      "  \"transformer.encoder.layers.2.linear2.weight\": 524288,\n",
      "  \"transformer.encoder.layers.2.linear2.bias\": 256,\n",
      "  \"transformer.encoder.layers.2.norm1.weight\": 256,\n",
      "  \"transformer.encoder.layers.2.norm1.bias\": 256,\n",
      "  \"transformer.encoder.layers.2.norm2.weight\": 256,\n",
      "  \"transformer.encoder.layers.2.norm2.bias\": 256,\n",
      "  \"transformer.encoder.layers.2.activation.weight\": 1,\n",
      "  \"transformer.encoder.layers.3.self_attn.in_proj_weight\": 196608,\n",
      "  \"transformer.encoder.layers.3.self_attn.in_proj_bias\": 768,\n",
      "  \"transformer.encoder.layers.3.self_attn.out_proj.weight\": 65536,\n",
      "  \"transformer.encoder.layers.3.self_attn.out_proj.bias\": 256,\n",
      "  \"transformer.encoder.layers.3.linear1.weight\": 524288,\n",
      "  \"transformer.encoder.layers.3.linear1.bias\": 2048,\n",
      "  \"transformer.encoder.layers.3.linear2.weight\": 524288,\n",
      "  \"transformer.encoder.layers.3.linear2.bias\": 256,\n",
      "  \"transformer.encoder.layers.3.norm1.weight\": 256,\n",
      "  \"transformer.encoder.layers.3.norm1.bias\": 256,\n",
      "  \"transformer.encoder.layers.3.norm2.weight\": 256,\n",
      "  \"transformer.encoder.layers.3.norm2.bias\": 256,\n",
      "  \"transformer.encoder.layers.3.activation.weight\": 1,\n",
      "  \"transformer.encoder.layers.4.self_attn.in_proj_weight\": 196608,\n",
      "  \"transformer.encoder.layers.4.self_attn.in_proj_bias\": 768,\n",
      "  \"transformer.encoder.layers.4.self_attn.out_proj.weight\": 65536,\n",
      "  \"transformer.encoder.layers.4.self_attn.out_proj.bias\": 256,\n",
      "  \"transformer.encoder.layers.4.linear1.weight\": 524288,\n",
      "  \"transformer.encoder.layers.4.linear1.bias\": 2048,\n",
      "  \"transformer.encoder.layers.4.linear2.weight\": 524288,\n",
      "  \"transformer.encoder.layers.4.linear2.bias\": 256,\n",
      "  \"transformer.encoder.layers.4.norm1.weight\": 256,\n",
      "  \"transformer.encoder.layers.4.norm1.bias\": 256,\n",
      "  \"transformer.encoder.layers.4.norm2.weight\": 256,\n",
      "  \"transformer.encoder.layers.4.norm2.bias\": 256,\n",
      "  \"transformer.encoder.layers.4.activation.weight\": 1,\n",
      "  \"transformer.encoder.layers.5.self_attn.in_proj_weight\": 196608,\n",
      "  \"transformer.encoder.layers.5.self_attn.in_proj_bias\": 768,\n",
      "  \"transformer.encoder.layers.5.self_attn.out_proj.weight\": 65536,\n",
      "  \"transformer.encoder.layers.5.self_attn.out_proj.bias\": 256,\n",
      "  \"transformer.encoder.layers.5.linear1.weight\": 524288,\n",
      "  \"transformer.encoder.layers.5.linear1.bias\": 2048,\n",
      "  \"transformer.encoder.layers.5.linear2.weight\": 524288,\n",
      "  \"transformer.encoder.layers.5.linear2.bias\": 256,\n",
      "  \"transformer.encoder.layers.5.norm1.weight\": 256,\n",
      "  \"transformer.encoder.layers.5.norm1.bias\": 256,\n",
      "  \"transformer.encoder.layers.5.norm2.weight\": 256,\n",
      "  \"transformer.encoder.layers.5.norm2.bias\": 256,\n",
      "  \"transformer.encoder.layers.5.activation.weight\": 1,\n",
      "  \"transformer.encoder.query_scale.layers.0.weight\": 65536,\n",
      "  \"transformer.encoder.query_scale.layers.0.bias\": 256,\n",
      "  \"transformer.encoder.query_scale.layers.1.weight\": 65536,\n",
      "  \"transformer.encoder.query_scale.layers.1.bias\": 256,\n",
      "  \"transformer.encoder.norm.weight\": 256,\n",
      "  \"transformer.encoder.norm.bias\": 256,\n",
      "  \"transformer.decoder.layers.0.sa_qcontent_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.0.sa_qcontent_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.0.sa_qpos_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.0.sa_qpos_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.0.sa_kcontent_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.0.sa_kcontent_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.0.sa_kpos_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.0.sa_kpos_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.0.sa_v_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.0.sa_v_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.0.self_attn.out_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.0.self_attn.out_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.0.norm1.weight\": 256,\n",
      "  \"transformer.decoder.layers.0.norm1.bias\": 256,\n",
      "  \"transformer.decoder.layers.0.ca_qcontent_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.0.ca_qcontent_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.0.ca_qpos_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.0.ca_qpos_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.0.ca_kcontent_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.0.ca_kcontent_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.0.ca_kpos_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.0.ca_kpos_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.0.ca_v_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.0.ca_v_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.0.ca_qpos_sine_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.0.ca_qpos_sine_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.0.cross_attn.out_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.0.cross_attn.out_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.0.linear1.weight\": 524288,\n",
      "  \"transformer.decoder.layers.0.linear1.bias\": 2048,\n",
      "  \"transformer.decoder.layers.0.linear2.weight\": 524288,\n",
      "  \"transformer.decoder.layers.0.linear2.bias\": 256,\n",
      "  \"transformer.decoder.layers.0.norm2.weight\": 256,\n",
      "  \"transformer.decoder.layers.0.norm2.bias\": 256,\n",
      "  \"transformer.decoder.layers.0.norm3.weight\": 256,\n",
      "  \"transformer.decoder.layers.0.norm3.bias\": 256,\n",
      "  \"transformer.decoder.layers.0.activation.weight\": 1,\n",
      "  \"transformer.decoder.layers.1.sa_qcontent_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.1.sa_qcontent_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.1.sa_qpos_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.1.sa_qpos_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.1.sa_kcontent_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.1.sa_kcontent_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.1.sa_kpos_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.1.sa_kpos_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.1.sa_v_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.1.sa_v_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.1.self_attn.out_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.1.self_attn.out_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.1.norm1.weight\": 256,\n",
      "  \"transformer.decoder.layers.1.norm1.bias\": 256,\n",
      "  \"transformer.decoder.layers.1.ca_qcontent_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.1.ca_qcontent_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.1.ca_kcontent_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.1.ca_kcontent_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.1.ca_kpos_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.1.ca_kpos_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.1.ca_v_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.1.ca_v_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.1.ca_qpos_sine_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.1.ca_qpos_sine_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.1.cross_attn.out_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.1.cross_attn.out_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.1.linear1.weight\": 524288,\n",
      "  \"transformer.decoder.layers.1.linear1.bias\": 2048,\n",
      "  \"transformer.decoder.layers.1.linear2.weight\": 524288,\n",
      "  \"transformer.decoder.layers.1.linear2.bias\": 256,\n",
      "  \"transformer.decoder.layers.1.norm2.weight\": 256,\n",
      "  \"transformer.decoder.layers.1.norm2.bias\": 256,\n",
      "  \"transformer.decoder.layers.1.norm3.weight\": 256,\n",
      "  \"transformer.decoder.layers.1.norm3.bias\": 256,\n",
      "  \"transformer.decoder.layers.1.activation.weight\": 1,\n",
      "  \"transformer.decoder.layers.2.sa_qcontent_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.2.sa_qcontent_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.2.sa_qpos_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.2.sa_qpos_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.2.sa_kcontent_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.2.sa_kcontent_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.2.sa_kpos_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.2.sa_kpos_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.2.sa_v_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.2.sa_v_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.2.self_attn.out_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.2.self_attn.out_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.2.norm1.weight\": 256,\n",
      "  \"transformer.decoder.layers.2.norm1.bias\": 256,\n",
      "  \"transformer.decoder.layers.2.ca_qcontent_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.2.ca_qcontent_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.2.ca_kcontent_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.2.ca_kcontent_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.2.ca_kpos_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.2.ca_kpos_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.2.ca_v_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.2.ca_v_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.2.ca_qpos_sine_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.2.ca_qpos_sine_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.2.cross_attn.out_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.2.cross_attn.out_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.2.linear1.weight\": 524288,\n",
      "  \"transformer.decoder.layers.2.linear1.bias\": 2048,\n",
      "  \"transformer.decoder.layers.2.linear2.weight\": 524288,\n",
      "  \"transformer.decoder.layers.2.linear2.bias\": 256,\n",
      "  \"transformer.decoder.layers.2.norm2.weight\": 256,\n",
      "  \"transformer.decoder.layers.2.norm2.bias\": 256,\n",
      "  \"transformer.decoder.layers.2.norm3.weight\": 256,\n",
      "  \"transformer.decoder.layers.2.norm3.bias\": 256,\n",
      "  \"transformer.decoder.layers.2.activation.weight\": 1,\n",
      "  \"transformer.decoder.layers.3.sa_qcontent_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.3.sa_qcontent_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.3.sa_qpos_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.3.sa_qpos_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.3.sa_kcontent_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.3.sa_kcontent_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.3.sa_kpos_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.3.sa_kpos_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.3.sa_v_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.3.sa_v_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.3.self_attn.out_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.3.self_attn.out_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.3.norm1.weight\": 256,\n",
      "  \"transformer.decoder.layers.3.norm1.bias\": 256,\n",
      "  \"transformer.decoder.layers.3.ca_qcontent_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.3.ca_qcontent_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.3.ca_kcontent_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.3.ca_kcontent_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.3.ca_kpos_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.3.ca_kpos_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.3.ca_v_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.3.ca_v_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.3.ca_qpos_sine_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.3.ca_qpos_sine_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.3.cross_attn.out_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.3.cross_attn.out_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.3.linear1.weight\": 524288,\n",
      "  \"transformer.decoder.layers.3.linear1.bias\": 2048,\n",
      "  \"transformer.decoder.layers.3.linear2.weight\": 524288,\n",
      "  \"transformer.decoder.layers.3.linear2.bias\": 256,\n",
      "  \"transformer.decoder.layers.3.norm2.weight\": 256,\n",
      "  \"transformer.decoder.layers.3.norm2.bias\": 256,\n",
      "  \"transformer.decoder.layers.3.norm3.weight\": 256,\n",
      "  \"transformer.decoder.layers.3.norm3.bias\": 256,\n",
      "  \"transformer.decoder.layers.3.activation.weight\": 1,\n",
      "  \"transformer.decoder.layers.4.sa_qcontent_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.4.sa_qcontent_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.4.sa_qpos_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.4.sa_qpos_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.4.sa_kcontent_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.4.sa_kcontent_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.4.sa_kpos_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.4.sa_kpos_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.4.sa_v_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.4.sa_v_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.4.self_attn.out_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.4.self_attn.out_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.4.norm1.weight\": 256,\n",
      "  \"transformer.decoder.layers.4.norm1.bias\": 256,\n",
      "  \"transformer.decoder.layers.4.ca_qcontent_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.4.ca_qcontent_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.4.ca_kcontent_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.4.ca_kcontent_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.4.ca_kpos_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.4.ca_kpos_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.4.ca_v_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.4.ca_v_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.4.ca_qpos_sine_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.4.ca_qpos_sine_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.4.cross_attn.out_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.4.cross_attn.out_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.4.linear1.weight\": 524288,\n",
      "  \"transformer.decoder.layers.4.linear1.bias\": 2048,\n",
      "  \"transformer.decoder.layers.4.linear2.weight\": 524288,\n",
      "  \"transformer.decoder.layers.4.linear2.bias\": 256,\n",
      "  \"transformer.decoder.layers.4.norm2.weight\": 256,\n",
      "  \"transformer.decoder.layers.4.norm2.bias\": 256,\n",
      "  \"transformer.decoder.layers.4.norm3.weight\": 256,\n",
      "  \"transformer.decoder.layers.4.norm3.bias\": 256,\n",
      "  \"transformer.decoder.layers.4.activation.weight\": 1,\n",
      "  \"transformer.decoder.layers.5.sa_qcontent_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.5.sa_qcontent_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.5.sa_qpos_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.5.sa_qpos_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.5.sa_kcontent_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.5.sa_kcontent_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.5.sa_kpos_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.5.sa_kpos_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.5.sa_v_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.5.sa_v_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.5.self_attn.out_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.5.self_attn.out_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.5.norm1.weight\": 256,\n",
      "  \"transformer.decoder.layers.5.norm1.bias\": 256,\n",
      "  \"transformer.decoder.layers.5.ca_qcontent_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.5.ca_qcontent_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.5.ca_kcontent_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.5.ca_kcontent_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.5.ca_kpos_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.5.ca_kpos_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.5.ca_v_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.5.ca_v_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.5.ca_qpos_sine_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.5.ca_qpos_sine_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.5.cross_attn.out_proj.weight\": 65536,\n",
      "  \"transformer.decoder.layers.5.cross_attn.out_proj.bias\": 256,\n",
      "  \"transformer.decoder.layers.5.linear1.weight\": 524288,\n",
      "  \"transformer.decoder.layers.5.linear1.bias\": 2048,\n",
      "  \"transformer.decoder.layers.5.linear2.weight\": 524288,\n",
      "  \"transformer.decoder.layers.5.linear2.bias\": 256,\n",
      "  \"transformer.decoder.layers.5.norm2.weight\": 256,\n",
      "  \"transformer.decoder.layers.5.norm2.bias\": 256,\n",
      "  \"transformer.decoder.layers.5.norm3.weight\": 256,\n",
      "  \"transformer.decoder.layers.5.norm3.bias\": 256,\n",
      "  \"transformer.decoder.layers.5.activation.weight\": 1,\n",
      "  \"transformer.decoder.norm.weight\": 256,\n",
      "  \"transformer.decoder.norm.bias\": 256,\n",
      "  \"transformer.decoder.query_scale.layers.0.weight\": 65536,\n",
      "  \"transformer.decoder.query_scale.layers.0.bias\": 256,\n",
      "  \"transformer.decoder.query_scale.layers.1.weight\": 65536,\n",
      "  \"transformer.decoder.query_scale.layers.1.bias\": 256,\n",
      "  \"transformer.decoder.ref_point_head.layers.0.weight\": 131072,\n",
      "  \"transformer.decoder.ref_point_head.layers.0.bias\": 256,\n",
      "  \"transformer.decoder.ref_point_head.layers.1.weight\": 65536,\n",
      "  \"transformer.decoder.ref_point_head.layers.1.bias\": 256,\n",
      "  \"transformer.decoder.ref_anchor_head.layers.0.weight\": 65536,\n",
      "  \"transformer.decoder.ref_anchor_head.layers.0.bias\": 256,\n",
      "  \"transformer.decoder.ref_anchor_head.layers.1.weight\": 512,\n",
      "  \"transformer.decoder.ref_anchor_head.layers.1.bias\": 2,\n",
      "  \"transformer.decoder.bbox_embed.layers.0.weight\": 65536,\n",
      "  \"transformer.decoder.bbox_embed.layers.0.bias\": 256,\n",
      "  \"transformer.decoder.bbox_embed.layers.1.weight\": 65536,\n",
      "  \"transformer.decoder.bbox_embed.layers.1.bias\": 256,\n",
      "  \"transformer.decoder.bbox_embed.layers.2.weight\": 1024,\n",
      "  \"transformer.decoder.bbox_embed.layers.2.bias\": 4,\n",
      "  \"class_embed.weight\": 23296,\n",
      "  \"class_embed.bias\": 91,\n",
      "  \"refpoint_embed.weight\": 1200,\n",
      "  \"input_proj.weight\": 524288,\n",
      "  \"input_proj.bias\": 256,\n",
      "  \"resnetbackbone.0.body.layer2.0.conv1.weight\": 32768,\n",
      "  \"resnetbackbone.0.body.layer2.0.conv2.weight\": 147456,\n",
      "  \"resnetbackbone.0.body.layer2.0.conv3.weight\": 65536,\n",
      "  \"resnetbackbone.0.body.layer2.0.downsample.0.weight\": 131072,\n",
      "  \"resnetbackbone.0.body.layer2.1.conv1.weight\": 65536,\n",
      "  \"resnetbackbone.0.body.layer2.1.conv2.weight\": 147456,\n",
      "  \"resnetbackbone.0.body.layer2.1.conv3.weight\": 65536,\n",
      "  \"resnetbackbone.0.body.layer2.2.conv1.weight\": 65536,\n",
      "  \"resnetbackbone.0.body.layer2.2.conv2.weight\": 147456,\n",
      "  \"resnetbackbone.0.body.layer2.2.conv3.weight\": 65536,\n",
      "  \"resnetbackbone.0.body.layer2.3.conv1.weight\": 65536,\n",
      "  \"resnetbackbone.0.body.layer2.3.conv2.weight\": 147456,\n",
      "  \"resnetbackbone.0.body.layer2.3.conv3.weight\": 65536,\n",
      "  \"resnetbackbone.0.body.layer3.0.conv1.weight\": 131072,\n",
      "  \"resnetbackbone.0.body.layer3.0.conv2.weight\": 589824,\n",
      "  \"resnetbackbone.0.body.layer3.0.conv3.weight\": 262144,\n",
      "  \"resnetbackbone.0.body.layer3.0.downsample.0.weight\": 524288,\n",
      "  \"resnetbackbone.0.body.layer3.1.conv1.weight\": 262144,\n",
      "  \"resnetbackbone.0.body.layer3.1.conv2.weight\": 589824,\n",
      "  \"resnetbackbone.0.body.layer3.1.conv3.weight\": 262144,\n",
      "  \"resnetbackbone.0.body.layer3.2.conv1.weight\": 262144,\n",
      "  \"resnetbackbone.0.body.layer3.2.conv2.weight\": 589824,\n",
      "  \"resnetbackbone.0.body.layer3.2.conv3.weight\": 262144,\n",
      "  \"resnetbackbone.0.body.layer3.3.conv1.weight\": 262144,\n",
      "  \"resnetbackbone.0.body.layer3.3.conv2.weight\": 589824,\n",
      "  \"resnetbackbone.0.body.layer3.3.conv3.weight\": 262144,\n",
      "  \"resnetbackbone.0.body.layer3.4.conv1.weight\": 262144,\n",
      "  \"resnetbackbone.0.body.layer3.4.conv2.weight\": 589824,\n",
      "  \"resnetbackbone.0.body.layer3.4.conv3.weight\": 262144,\n",
      "  \"resnetbackbone.0.body.layer3.5.conv1.weight\": 262144,\n",
      "  \"resnetbackbone.0.body.layer3.5.conv2.weight\": 589824,\n",
      "  \"resnetbackbone.0.body.layer3.5.conv3.weight\": 262144,\n",
      "  \"resnetbackbone.0.body.layer4.0.conv1.weight\": 524288,\n",
      "  \"resnetbackbone.0.body.layer4.0.conv2.weight\": 2359296,\n",
      "  \"resnetbackbone.0.body.layer4.0.conv3.weight\": 1048576,\n",
      "  \"resnetbackbone.0.body.layer4.0.downsample.0.weight\": 2097152,\n",
      "  \"resnetbackbone.0.body.layer4.1.conv1.weight\": 1048576,\n",
      "  \"resnetbackbone.0.body.layer4.1.conv2.weight\": 2359296,\n",
      "  \"resnetbackbone.0.body.layer4.1.conv3.weight\": 1048576,\n",
      "  \"resnetbackbone.0.body.layer4.2.conv1.weight\": 1048576,\n",
      "  \"resnetbackbone.0.body.layer4.2.conv2.weight\": 2359296,\n",
      "  \"resnetbackbone.0.body.layer4.2.conv3.weight\": 1048576\n",
      "}\n",
      "loading annotations into memory...\n",
      "Done (t=43.43s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=1.76s)\n",
      "creating index...\n",
      "index created!\n",
      "[03/14 11:28:45.625]: Ignore keys: []\n",
      "[03/14 11:28:45.880]: _IncompatibleKeys(missing_keys=['transformer.encoder.norm.weight', 'transformer.encoder.norm.bias', 'resnetbackbone.0.body.conv1.weight', 'resnetbackbone.0.body.bn1.weight', 'resnetbackbone.0.body.bn1.bias', 'resnetbackbone.0.body.bn1.running_mean', 'resnetbackbone.0.body.bn1.running_var', 'resnetbackbone.0.body.layer1.0.conv1.weight', 'resnetbackbone.0.body.layer1.0.bn1.weight', 'resnetbackbone.0.body.layer1.0.bn1.bias', 'resnetbackbone.0.body.layer1.0.bn1.running_mean', 'resnetbackbone.0.body.layer1.0.bn1.running_var', 'resnetbackbone.0.body.layer1.0.conv2.weight', 'resnetbackbone.0.body.layer1.0.bn2.weight', 'resnetbackbone.0.body.layer1.0.bn2.bias', 'resnetbackbone.0.body.layer1.0.bn2.running_mean', 'resnetbackbone.0.body.layer1.0.bn2.running_var', 'resnetbackbone.0.body.layer1.0.conv3.weight', 'resnetbackbone.0.body.layer1.0.bn3.weight', 'resnetbackbone.0.body.layer1.0.bn3.bias', 'resnetbackbone.0.body.layer1.0.bn3.running_mean', 'resnetbackbone.0.body.layer1.0.bn3.running_var', 'resnetbackbone.0.body.layer1.0.downsample.0.weight', 'resnetbackbone.0.body.layer1.0.downsample.1.weight', 'resnetbackbone.0.body.layer1.0.downsample.1.bias', 'resnetbackbone.0.body.layer1.0.downsample.1.running_mean', 'resnetbackbone.0.body.layer1.0.downsample.1.running_var', 'resnetbackbone.0.body.layer1.1.conv1.weight', 'resnetbackbone.0.body.layer1.1.bn1.weight', 'resnetbackbone.0.body.layer1.1.bn1.bias', 'resnetbackbone.0.body.layer1.1.bn1.running_mean', 'resnetbackbone.0.body.layer1.1.bn1.running_var', 'resnetbackbone.0.body.layer1.1.conv2.weight', 'resnetbackbone.0.body.layer1.1.bn2.weight', 'resnetbackbone.0.body.layer1.1.bn2.bias', 'resnetbackbone.0.body.layer1.1.bn2.running_mean', 'resnetbackbone.0.body.layer1.1.bn2.running_var', 'resnetbackbone.0.body.layer1.1.conv3.weight', 'resnetbackbone.0.body.layer1.1.bn3.weight', 'resnetbackbone.0.body.layer1.1.bn3.bias', 'resnetbackbone.0.body.layer1.1.bn3.running_mean', 'resnetbackbone.0.body.layer1.1.bn3.running_var', 'resnetbackbone.0.body.layer1.2.conv1.weight', 'resnetbackbone.0.body.layer1.2.bn1.weight', 'resnetbackbone.0.body.layer1.2.bn1.bias', 'resnetbackbone.0.body.layer1.2.bn1.running_mean', 'resnetbackbone.0.body.layer1.2.bn1.running_var', 'resnetbackbone.0.body.layer1.2.conv2.weight', 'resnetbackbone.0.body.layer1.2.bn2.weight', 'resnetbackbone.0.body.layer1.2.bn2.bias', 'resnetbackbone.0.body.layer1.2.bn2.running_mean', 'resnetbackbone.0.body.layer1.2.bn2.running_var', 'resnetbackbone.0.body.layer1.2.conv3.weight', 'resnetbackbone.0.body.layer1.2.bn3.weight', 'resnetbackbone.0.body.layer1.2.bn3.bias', 'resnetbackbone.0.body.layer1.2.bn3.running_mean', 'resnetbackbone.0.body.layer1.2.bn3.running_var', 'resnetbackbone.0.body.layer2.0.conv1.weight', 'resnetbackbone.0.body.layer2.0.bn1.weight', 'resnetbackbone.0.body.layer2.0.bn1.bias', 'resnetbackbone.0.body.layer2.0.bn1.running_mean', 'resnetbackbone.0.body.layer2.0.bn1.running_var', 'resnetbackbone.0.body.layer2.0.conv2.weight', 'resnetbackbone.0.body.layer2.0.bn2.weight', 'resnetbackbone.0.body.layer2.0.bn2.bias', 'resnetbackbone.0.body.layer2.0.bn2.running_mean', 'resnetbackbone.0.body.layer2.0.bn2.running_var', 'resnetbackbone.0.body.layer2.0.conv3.weight', 'resnetbackbone.0.body.layer2.0.bn3.weight', 'resnetbackbone.0.body.layer2.0.bn3.bias', 'resnetbackbone.0.body.layer2.0.bn3.running_mean', 'resnetbackbone.0.body.layer2.0.bn3.running_var', 'resnetbackbone.0.body.layer2.0.downsample.0.weight', 'resnetbackbone.0.body.layer2.0.downsample.1.weight', 'resnetbackbone.0.body.layer2.0.downsample.1.bias', 'resnetbackbone.0.body.layer2.0.downsample.1.running_mean', 'resnetbackbone.0.body.layer2.0.downsample.1.running_var', 'resnetbackbone.0.body.layer2.1.conv1.weight', 'resnetbackbone.0.body.layer2.1.bn1.weight', 'resnetbackbone.0.body.layer2.1.bn1.bias', 'resnetbackbone.0.body.layer2.1.bn1.running_mean', 'resnetbackbone.0.body.layer2.1.bn1.running_var', 'resnetbackbone.0.body.layer2.1.conv2.weight', 'resnetbackbone.0.body.layer2.1.bn2.weight', 'resnetbackbone.0.body.layer2.1.bn2.bias', 'resnetbackbone.0.body.layer2.1.bn2.running_mean', 'resnetbackbone.0.body.layer2.1.bn2.running_var', 'resnetbackbone.0.body.layer2.1.conv3.weight', 'resnetbackbone.0.body.layer2.1.bn3.weight', 'resnetbackbone.0.body.layer2.1.bn3.bias', 'resnetbackbone.0.body.layer2.1.bn3.running_mean', 'resnetbackbone.0.body.layer2.1.bn3.running_var', 'resnetbackbone.0.body.layer2.2.conv1.weight', 'resnetbackbone.0.body.layer2.2.bn1.weight', 'resnetbackbone.0.body.layer2.2.bn1.bias', 'resnetbackbone.0.body.layer2.2.bn1.running_mean', 'resnetbackbone.0.body.layer2.2.bn1.running_var', 'resnetbackbone.0.body.layer2.2.conv2.weight', 'resnetbackbone.0.body.layer2.2.bn2.weight', 'resnetbackbone.0.body.layer2.2.bn2.bias', 'resnetbackbone.0.body.layer2.2.bn2.running_mean', 'resnetbackbone.0.body.layer2.2.bn2.running_var', 'resnetbackbone.0.body.layer2.2.conv3.weight', 'resnetbackbone.0.body.layer2.2.bn3.weight', 'resnetbackbone.0.body.layer2.2.bn3.bias', 'resnetbackbone.0.body.layer2.2.bn3.running_mean', 'resnetbackbone.0.body.layer2.2.bn3.running_var', 'resnetbackbone.0.body.layer2.3.conv1.weight', 'resnetbackbone.0.body.layer2.3.bn1.weight', 'resnetbackbone.0.body.layer2.3.bn1.bias', 'resnetbackbone.0.body.layer2.3.bn1.running_mean', 'resnetbackbone.0.body.layer2.3.bn1.running_var', 'resnetbackbone.0.body.layer2.3.conv2.weight', 'resnetbackbone.0.body.layer2.3.bn2.weight', 'resnetbackbone.0.body.layer2.3.bn2.bias', 'resnetbackbone.0.body.layer2.3.bn2.running_mean', 'resnetbackbone.0.body.layer2.3.bn2.running_var', 'resnetbackbone.0.body.layer2.3.conv3.weight', 'resnetbackbone.0.body.layer2.3.bn3.weight', 'resnetbackbone.0.body.layer2.3.bn3.bias', 'resnetbackbone.0.body.layer2.3.bn3.running_mean', 'resnetbackbone.0.body.layer2.3.bn3.running_var', 'resnetbackbone.0.body.layer3.0.conv1.weight', 'resnetbackbone.0.body.layer3.0.bn1.weight', 'resnetbackbone.0.body.layer3.0.bn1.bias', 'resnetbackbone.0.body.layer3.0.bn1.running_mean', 'resnetbackbone.0.body.layer3.0.bn1.running_var', 'resnetbackbone.0.body.layer3.0.conv2.weight', 'resnetbackbone.0.body.layer3.0.bn2.weight', 'resnetbackbone.0.body.layer3.0.bn2.bias', 'resnetbackbone.0.body.layer3.0.bn2.running_mean', 'resnetbackbone.0.body.layer3.0.bn2.running_var', 'resnetbackbone.0.body.layer3.0.conv3.weight', 'resnetbackbone.0.body.layer3.0.bn3.weight', 'resnetbackbone.0.body.layer3.0.bn3.bias', 'resnetbackbone.0.body.layer3.0.bn3.running_mean', 'resnetbackbone.0.body.layer3.0.bn3.running_var', 'resnetbackbone.0.body.layer3.0.downsample.0.weight', 'resnetbackbone.0.body.layer3.0.downsample.1.weight', 'resnetbackbone.0.body.layer3.0.downsample.1.bias', 'resnetbackbone.0.body.layer3.0.downsample.1.running_mean', 'resnetbackbone.0.body.layer3.0.downsample.1.running_var', 'resnetbackbone.0.body.layer3.1.conv1.weight', 'resnetbackbone.0.body.layer3.1.bn1.weight', 'resnetbackbone.0.body.layer3.1.bn1.bias', 'resnetbackbone.0.body.layer3.1.bn1.running_mean', 'resnetbackbone.0.body.layer3.1.bn1.running_var', 'resnetbackbone.0.body.layer3.1.conv2.weight', 'resnetbackbone.0.body.layer3.1.bn2.weight', 'resnetbackbone.0.body.layer3.1.bn2.bias', 'resnetbackbone.0.body.layer3.1.bn2.running_mean', 'resnetbackbone.0.body.layer3.1.bn2.running_var', 'resnetbackbone.0.body.layer3.1.conv3.weight', 'resnetbackbone.0.body.layer3.1.bn3.weight', 'resnetbackbone.0.body.layer3.1.bn3.bias', 'resnetbackbone.0.body.layer3.1.bn3.running_mean', 'resnetbackbone.0.body.layer3.1.bn3.running_var', 'resnetbackbone.0.body.layer3.2.conv1.weight', 'resnetbackbone.0.body.layer3.2.bn1.weight', 'resnetbackbone.0.body.layer3.2.bn1.bias', 'resnetbackbone.0.body.layer3.2.bn1.running_mean', 'resnetbackbone.0.body.layer3.2.bn1.running_var', 'resnetbackbone.0.body.layer3.2.conv2.weight', 'resnetbackbone.0.body.layer3.2.bn2.weight', 'resnetbackbone.0.body.layer3.2.bn2.bias', 'resnetbackbone.0.body.layer3.2.bn2.running_mean', 'resnetbackbone.0.body.layer3.2.bn2.running_var', 'resnetbackbone.0.body.layer3.2.conv3.weight', 'resnetbackbone.0.body.layer3.2.bn3.weight', 'resnetbackbone.0.body.layer3.2.bn3.bias', 'resnetbackbone.0.body.layer3.2.bn3.running_mean', 'resnetbackbone.0.body.layer3.2.bn3.running_var', 'resnetbackbone.0.body.layer3.3.conv1.weight', 'resnetbackbone.0.body.layer3.3.bn1.weight', 'resnetbackbone.0.body.layer3.3.bn1.bias', 'resnetbackbone.0.body.layer3.3.bn1.running_mean', 'resnetbackbone.0.body.layer3.3.bn1.running_var', 'resnetbackbone.0.body.layer3.3.conv2.weight', 'resnetbackbone.0.body.layer3.3.bn2.weight', 'resnetbackbone.0.body.layer3.3.bn2.bias', 'resnetbackbone.0.body.layer3.3.bn2.running_mean', 'resnetbackbone.0.body.layer3.3.bn2.running_var', 'resnetbackbone.0.body.layer3.3.conv3.weight', 'resnetbackbone.0.body.layer3.3.bn3.weight', 'resnetbackbone.0.body.layer3.3.bn3.bias', 'resnetbackbone.0.body.layer3.3.bn3.running_mean', 'resnetbackbone.0.body.layer3.3.bn3.running_var', 'resnetbackbone.0.body.layer3.4.conv1.weight', 'resnetbackbone.0.body.layer3.4.bn1.weight', 'resnetbackbone.0.body.layer3.4.bn1.bias', 'resnetbackbone.0.body.layer3.4.bn1.running_mean', 'resnetbackbone.0.body.layer3.4.bn1.running_var', 'resnetbackbone.0.body.layer3.4.conv2.weight', 'resnetbackbone.0.body.layer3.4.bn2.weight', 'resnetbackbone.0.body.layer3.4.bn2.bias', 'resnetbackbone.0.body.layer3.4.bn2.running_mean', 'resnetbackbone.0.body.layer3.4.bn2.running_var', 'resnetbackbone.0.body.layer3.4.conv3.weight', 'resnetbackbone.0.body.layer3.4.bn3.weight', 'resnetbackbone.0.body.layer3.4.bn3.bias', 'resnetbackbone.0.body.layer3.4.bn3.running_mean', 'resnetbackbone.0.body.layer3.4.bn3.running_var', 'resnetbackbone.0.body.layer3.5.conv1.weight', 'resnetbackbone.0.body.layer3.5.bn1.weight', 'resnetbackbone.0.body.layer3.5.bn1.bias', 'resnetbackbone.0.body.layer3.5.bn1.running_mean', 'resnetbackbone.0.body.layer3.5.bn1.running_var', 'resnetbackbone.0.body.layer3.5.conv2.weight', 'resnetbackbone.0.body.layer3.5.bn2.weight', 'resnetbackbone.0.body.layer3.5.bn2.bias', 'resnetbackbone.0.body.layer3.5.bn2.running_mean', 'resnetbackbone.0.body.layer3.5.bn2.running_var', 'resnetbackbone.0.body.layer3.5.conv3.weight', 'resnetbackbone.0.body.layer3.5.bn3.weight', 'resnetbackbone.0.body.layer3.5.bn3.bias', 'resnetbackbone.0.body.layer3.5.bn3.running_mean', 'resnetbackbone.0.body.layer3.5.bn3.running_var', 'resnetbackbone.0.body.layer4.0.conv1.weight', 'resnetbackbone.0.body.layer4.0.bn1.weight', 'resnetbackbone.0.body.layer4.0.bn1.bias', 'resnetbackbone.0.body.layer4.0.bn1.running_mean', 'resnetbackbone.0.body.layer4.0.bn1.running_var', 'resnetbackbone.0.body.layer4.0.conv2.weight', 'resnetbackbone.0.body.layer4.0.bn2.weight', 'resnetbackbone.0.body.layer4.0.bn2.bias', 'resnetbackbone.0.body.layer4.0.bn2.running_mean', 'resnetbackbone.0.body.layer4.0.bn2.running_var', 'resnetbackbone.0.body.layer4.0.conv3.weight', 'resnetbackbone.0.body.layer4.0.bn3.weight', 'resnetbackbone.0.body.layer4.0.bn3.bias', 'resnetbackbone.0.body.layer4.0.bn3.running_mean', 'resnetbackbone.0.body.layer4.0.bn3.running_var', 'resnetbackbone.0.body.layer4.0.downsample.0.weight', 'resnetbackbone.0.body.layer4.0.downsample.1.weight', 'resnetbackbone.0.body.layer4.0.downsample.1.bias', 'resnetbackbone.0.body.layer4.0.downsample.1.running_mean', 'resnetbackbone.0.body.layer4.0.downsample.1.running_var', 'resnetbackbone.0.body.layer4.1.conv1.weight', 'resnetbackbone.0.body.layer4.1.bn1.weight', 'resnetbackbone.0.body.layer4.1.bn1.bias', 'resnetbackbone.0.body.layer4.1.bn1.running_mean', 'resnetbackbone.0.body.layer4.1.bn1.running_var', 'resnetbackbone.0.body.layer4.1.conv2.weight', 'resnetbackbone.0.body.layer4.1.bn2.weight', 'resnetbackbone.0.body.layer4.1.bn2.bias', 'resnetbackbone.0.body.layer4.1.bn2.running_mean', 'resnetbackbone.0.body.layer4.1.bn2.running_var', 'resnetbackbone.0.body.layer4.1.conv3.weight', 'resnetbackbone.0.body.layer4.1.bn3.weight', 'resnetbackbone.0.body.layer4.1.bn3.bias', 'resnetbackbone.0.body.layer4.1.bn3.running_mean', 'resnetbackbone.0.body.layer4.1.bn3.running_var', 'resnetbackbone.0.body.layer4.2.conv1.weight', 'resnetbackbone.0.body.layer4.2.bn1.weight', 'resnetbackbone.0.body.layer4.2.bn1.bias', 'resnetbackbone.0.body.layer4.2.bn1.running_mean', 'resnetbackbone.0.body.layer4.2.bn1.running_var', 'resnetbackbone.0.body.layer4.2.conv2.weight', 'resnetbackbone.0.body.layer4.2.bn2.weight', 'resnetbackbone.0.body.layer4.2.bn2.bias', 'resnetbackbone.0.body.layer4.2.bn2.running_mean', 'resnetbackbone.0.body.layer4.2.bn2.running_var', 'resnetbackbone.0.body.layer4.2.conv3.weight', 'resnetbackbone.0.body.layer4.2.bn3.weight', 'resnetbackbone.0.body.layer4.2.bn3.bias', 'resnetbackbone.0.body.layer4.2.bn3.running_mean', 'resnetbackbone.0.body.layer4.2.bn3.running_var'], unexpected_keys=['backbone.0.body.conv1.weight', 'backbone.0.body.bn1.weight', 'backbone.0.body.bn1.bias', 'backbone.0.body.bn1.running_mean', 'backbone.0.body.bn1.running_var', 'backbone.0.body.layer1.0.conv1.weight', 'backbone.0.body.layer1.0.bn1.weight', 'backbone.0.body.layer1.0.bn1.bias', 'backbone.0.body.layer1.0.bn1.running_mean', 'backbone.0.body.layer1.0.bn1.running_var', 'backbone.0.body.layer1.0.conv2.weight', 'backbone.0.body.layer1.0.bn2.weight', 'backbone.0.body.layer1.0.bn2.bias', 'backbone.0.body.layer1.0.bn2.running_mean', 'backbone.0.body.layer1.0.bn2.running_var', 'backbone.0.body.layer1.0.conv3.weight', 'backbone.0.body.layer1.0.bn3.weight', 'backbone.0.body.layer1.0.bn3.bias', 'backbone.0.body.layer1.0.bn3.running_mean', 'backbone.0.body.layer1.0.bn3.running_var', 'backbone.0.body.layer1.0.downsample.0.weight', 'backbone.0.body.layer1.0.downsample.1.weight', 'backbone.0.body.layer1.0.downsample.1.bias', 'backbone.0.body.layer1.0.downsample.1.running_mean', 'backbone.0.body.layer1.0.downsample.1.running_var', 'backbone.0.body.layer1.1.conv1.weight', 'backbone.0.body.layer1.1.bn1.weight', 'backbone.0.body.layer1.1.bn1.bias', 'backbone.0.body.layer1.1.bn1.running_mean', 'backbone.0.body.layer1.1.bn1.running_var', 'backbone.0.body.layer1.1.conv2.weight', 'backbone.0.body.layer1.1.bn2.weight', 'backbone.0.body.layer1.1.bn2.bias', 'backbone.0.body.layer1.1.bn2.running_mean', 'backbone.0.body.layer1.1.bn2.running_var', 'backbone.0.body.layer1.1.conv3.weight', 'backbone.0.body.layer1.1.bn3.weight', 'backbone.0.body.layer1.1.bn3.bias', 'backbone.0.body.layer1.1.bn3.running_mean', 'backbone.0.body.layer1.1.bn3.running_var', 'backbone.0.body.layer1.2.conv1.weight', 'backbone.0.body.layer1.2.bn1.weight', 'backbone.0.body.layer1.2.bn1.bias', 'backbone.0.body.layer1.2.bn1.running_mean', 'backbone.0.body.layer1.2.bn1.running_var', 'backbone.0.body.layer1.2.conv2.weight', 'backbone.0.body.layer1.2.bn2.weight', 'backbone.0.body.layer1.2.bn2.bias', 'backbone.0.body.layer1.2.bn2.running_mean', 'backbone.0.body.layer1.2.bn2.running_var', 'backbone.0.body.layer1.2.conv3.weight', 'backbone.0.body.layer1.2.bn3.weight', 'backbone.0.body.layer1.2.bn3.bias', 'backbone.0.body.layer1.2.bn3.running_mean', 'backbone.0.body.layer1.2.bn3.running_var', 'backbone.0.body.layer2.0.conv1.weight', 'backbone.0.body.layer2.0.bn1.weight', 'backbone.0.body.layer2.0.bn1.bias', 'backbone.0.body.layer2.0.bn1.running_mean', 'backbone.0.body.layer2.0.bn1.running_var', 'backbone.0.body.layer2.0.conv2.weight', 'backbone.0.body.layer2.0.bn2.weight', 'backbone.0.body.layer2.0.bn2.bias', 'backbone.0.body.layer2.0.bn2.running_mean', 'backbone.0.body.layer2.0.bn2.running_var', 'backbone.0.body.layer2.0.conv3.weight', 'backbone.0.body.layer2.0.bn3.weight', 'backbone.0.body.layer2.0.bn3.bias', 'backbone.0.body.layer2.0.bn3.running_mean', 'backbone.0.body.layer2.0.bn3.running_var', 'backbone.0.body.layer2.0.downsample.0.weight', 'backbone.0.body.layer2.0.downsample.1.weight', 'backbone.0.body.layer2.0.downsample.1.bias', 'backbone.0.body.layer2.0.downsample.1.running_mean', 'backbone.0.body.layer2.0.downsample.1.running_var', 'backbone.0.body.layer2.1.conv1.weight', 'backbone.0.body.layer2.1.bn1.weight', 'backbone.0.body.layer2.1.bn1.bias', 'backbone.0.body.layer2.1.bn1.running_mean', 'backbone.0.body.layer2.1.bn1.running_var', 'backbone.0.body.layer2.1.conv2.weight', 'backbone.0.body.layer2.1.bn2.weight', 'backbone.0.body.layer2.1.bn2.bias', 'backbone.0.body.layer2.1.bn2.running_mean', 'backbone.0.body.layer2.1.bn2.running_var', 'backbone.0.body.layer2.1.conv3.weight', 'backbone.0.body.layer2.1.bn3.weight', 'backbone.0.body.layer2.1.bn3.bias', 'backbone.0.body.layer2.1.bn3.running_mean', 'backbone.0.body.layer2.1.bn3.running_var', 'backbone.0.body.layer2.2.conv1.weight', 'backbone.0.body.layer2.2.bn1.weight', 'backbone.0.body.layer2.2.bn1.bias', 'backbone.0.body.layer2.2.bn1.running_mean', 'backbone.0.body.layer2.2.bn1.running_var', 'backbone.0.body.layer2.2.conv2.weight', 'backbone.0.body.layer2.2.bn2.weight', 'backbone.0.body.layer2.2.bn2.bias', 'backbone.0.body.layer2.2.bn2.running_mean', 'backbone.0.body.layer2.2.bn2.running_var', 'backbone.0.body.layer2.2.conv3.weight', 'backbone.0.body.layer2.2.bn3.weight', 'backbone.0.body.layer2.2.bn3.bias', 'backbone.0.body.layer2.2.bn3.running_mean', 'backbone.0.body.layer2.2.bn3.running_var', 'backbone.0.body.layer2.3.conv1.weight', 'backbone.0.body.layer2.3.bn1.weight', 'backbone.0.body.layer2.3.bn1.bias', 'backbone.0.body.layer2.3.bn1.running_mean', 'backbone.0.body.layer2.3.bn1.running_var', 'backbone.0.body.layer2.3.conv2.weight', 'backbone.0.body.layer2.3.bn2.weight', 'backbone.0.body.layer2.3.bn2.bias', 'backbone.0.body.layer2.3.bn2.running_mean', 'backbone.0.body.layer2.3.bn2.running_var', 'backbone.0.body.layer2.3.conv3.weight', 'backbone.0.body.layer2.3.bn3.weight', 'backbone.0.body.layer2.3.bn3.bias', 'backbone.0.body.layer2.3.bn3.running_mean', 'backbone.0.body.layer2.3.bn3.running_var', 'backbone.0.body.layer3.0.conv1.weight', 'backbone.0.body.layer3.0.bn1.weight', 'backbone.0.body.layer3.0.bn1.bias', 'backbone.0.body.layer3.0.bn1.running_mean', 'backbone.0.body.layer3.0.bn1.running_var', 'backbone.0.body.layer3.0.conv2.weight', 'backbone.0.body.layer3.0.bn2.weight', 'backbone.0.body.layer3.0.bn2.bias', 'backbone.0.body.layer3.0.bn2.running_mean', 'backbone.0.body.layer3.0.bn2.running_var', 'backbone.0.body.layer3.0.conv3.weight', 'backbone.0.body.layer3.0.bn3.weight', 'backbone.0.body.layer3.0.bn3.bias', 'backbone.0.body.layer3.0.bn3.running_mean', 'backbone.0.body.layer3.0.bn3.running_var', 'backbone.0.body.layer3.0.downsample.0.weight', 'backbone.0.body.layer3.0.downsample.1.weight', 'backbone.0.body.layer3.0.downsample.1.bias', 'backbone.0.body.layer3.0.downsample.1.running_mean', 'backbone.0.body.layer3.0.downsample.1.running_var', 'backbone.0.body.layer3.1.conv1.weight', 'backbone.0.body.layer3.1.bn1.weight', 'backbone.0.body.layer3.1.bn1.bias', 'backbone.0.body.layer3.1.bn1.running_mean', 'backbone.0.body.layer3.1.bn1.running_var', 'backbone.0.body.layer3.1.conv2.weight', 'backbone.0.body.layer3.1.bn2.weight', 'backbone.0.body.layer3.1.bn2.bias', 'backbone.0.body.layer3.1.bn2.running_mean', 'backbone.0.body.layer3.1.bn2.running_var', 'backbone.0.body.layer3.1.conv3.weight', 'backbone.0.body.layer3.1.bn3.weight', 'backbone.0.body.layer3.1.bn3.bias', 'backbone.0.body.layer3.1.bn3.running_mean', 'backbone.0.body.layer3.1.bn3.running_var', 'backbone.0.body.layer3.2.conv1.weight', 'backbone.0.body.layer3.2.bn1.weight', 'backbone.0.body.layer3.2.bn1.bias', 'backbone.0.body.layer3.2.bn1.running_mean', 'backbone.0.body.layer3.2.bn1.running_var', 'backbone.0.body.layer3.2.conv2.weight', 'backbone.0.body.layer3.2.bn2.weight', 'backbone.0.body.layer3.2.bn2.bias', 'backbone.0.body.layer3.2.bn2.running_mean', 'backbone.0.body.layer3.2.bn2.running_var', 'backbone.0.body.layer3.2.conv3.weight', 'backbone.0.body.layer3.2.bn3.weight', 'backbone.0.body.layer3.2.bn3.bias', 'backbone.0.body.layer3.2.bn3.running_mean', 'backbone.0.body.layer3.2.bn3.running_var', 'backbone.0.body.layer3.3.conv1.weight', 'backbone.0.body.layer3.3.bn1.weight', 'backbone.0.body.layer3.3.bn1.bias', 'backbone.0.body.layer3.3.bn1.running_mean', 'backbone.0.body.layer3.3.bn1.running_var', 'backbone.0.body.layer3.3.conv2.weight', 'backbone.0.body.layer3.3.bn2.weight', 'backbone.0.body.layer3.3.bn2.bias', 'backbone.0.body.layer3.3.bn2.running_mean', 'backbone.0.body.layer3.3.bn2.running_var', 'backbone.0.body.layer3.3.conv3.weight', 'backbone.0.body.layer3.3.bn3.weight', 'backbone.0.body.layer3.3.bn3.bias', 'backbone.0.body.layer3.3.bn3.running_mean', 'backbone.0.body.layer3.3.bn3.running_var', 'backbone.0.body.layer3.4.conv1.weight', 'backbone.0.body.layer3.4.bn1.weight', 'backbone.0.body.layer3.4.bn1.bias', 'backbone.0.body.layer3.4.bn1.running_mean', 'backbone.0.body.layer3.4.bn1.running_var', 'backbone.0.body.layer3.4.conv2.weight', 'backbone.0.body.layer3.4.bn2.weight', 'backbone.0.body.layer3.4.bn2.bias', 'backbone.0.body.layer3.4.bn2.running_mean', 'backbone.0.body.layer3.4.bn2.running_var', 'backbone.0.body.layer3.4.conv3.weight', 'backbone.0.body.layer3.4.bn3.weight', 'backbone.0.body.layer3.4.bn3.bias', 'backbone.0.body.layer3.4.bn3.running_mean', 'backbone.0.body.layer3.4.bn3.running_var', 'backbone.0.body.layer3.5.conv1.weight', 'backbone.0.body.layer3.5.bn1.weight', 'backbone.0.body.layer3.5.bn1.bias', 'backbone.0.body.layer3.5.bn1.running_mean', 'backbone.0.body.layer3.5.bn1.running_var', 'backbone.0.body.layer3.5.conv2.weight', 'backbone.0.body.layer3.5.bn2.weight', 'backbone.0.body.layer3.5.bn2.bias', 'backbone.0.body.layer3.5.bn2.running_mean', 'backbone.0.body.layer3.5.bn2.running_var', 'backbone.0.body.layer3.5.conv3.weight', 'backbone.0.body.layer3.5.bn3.weight', 'backbone.0.body.layer3.5.bn3.bias', 'backbone.0.body.layer3.5.bn3.running_mean', 'backbone.0.body.layer3.5.bn3.running_var', 'backbone.0.body.layer4.0.conv1.weight', 'backbone.0.body.layer4.0.bn1.weight', 'backbone.0.body.layer4.0.bn1.bias', 'backbone.0.body.layer4.0.bn1.running_mean', 'backbone.0.body.layer4.0.bn1.running_var', 'backbone.0.body.layer4.0.conv2.weight', 'backbone.0.body.layer4.0.bn2.weight', 'backbone.0.body.layer4.0.bn2.bias', 'backbone.0.body.layer4.0.bn2.running_mean', 'backbone.0.body.layer4.0.bn2.running_var', 'backbone.0.body.layer4.0.conv3.weight', 'backbone.0.body.layer4.0.bn3.weight', 'backbone.0.body.layer4.0.bn3.bias', 'backbone.0.body.layer4.0.bn3.running_mean', 'backbone.0.body.layer4.0.bn3.running_var', 'backbone.0.body.layer4.0.downsample.0.weight', 'backbone.0.body.layer4.0.downsample.1.weight', 'backbone.0.body.layer4.0.downsample.1.bias', 'backbone.0.body.layer4.0.downsample.1.running_mean', 'backbone.0.body.layer4.0.downsample.1.running_var', 'backbone.0.body.layer4.1.conv1.weight', 'backbone.0.body.layer4.1.bn1.weight', 'backbone.0.body.layer4.1.bn1.bias', 'backbone.0.body.layer4.1.bn1.running_mean', 'backbone.0.body.layer4.1.bn1.running_var', 'backbone.0.body.layer4.1.conv2.weight', 'backbone.0.body.layer4.1.bn2.weight', 'backbone.0.body.layer4.1.bn2.bias', 'backbone.0.body.layer4.1.bn2.running_mean', 'backbone.0.body.layer4.1.bn2.running_var', 'backbone.0.body.layer4.1.conv3.weight', 'backbone.0.body.layer4.1.bn3.weight', 'backbone.0.body.layer4.1.bn3.bias', 'backbone.0.body.layer4.1.bn3.running_mean', 'backbone.0.body.layer4.1.bn3.running_var', 'backbone.0.body.layer4.2.conv1.weight', 'backbone.0.body.layer4.2.bn1.weight', 'backbone.0.body.layer4.2.bn1.bias', 'backbone.0.body.layer4.2.bn1.running_mean', 'backbone.0.body.layer4.2.bn1.running_var', 'backbone.0.body.layer4.2.conv2.weight', 'backbone.0.body.layer4.2.bn2.weight', 'backbone.0.body.layer4.2.bn2.bias', 'backbone.0.body.layer4.2.bn2.running_mean', 'backbone.0.body.layer4.2.bn2.running_var', 'backbone.0.body.layer4.2.conv3.weight', 'backbone.0.body.layer4.2.bn3.weight', 'backbone.0.body.layer4.2.bn3.bias', 'backbone.0.body.layer4.2.bn3.running_mean', 'backbone.0.body.layer4.2.bn3.running_var'])\n",
      "Start eval loop\n"
     ]
    }
   ],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "from util.logger import setup_logger\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "import torch.distributed as dist\n",
    "\n",
    "\n",
    "import util.misc as utils\n",
    "from util.utils import clean_state_dict\n",
    "\n",
    "\n",
    "def build_model_main(modelname):\n",
    "    if modelname.lower() == 'dab_detr':\n",
    "        model, criterion, postprocessors = build_DABDETR(dataset_file = 'coco', device = 'cuda', cls_loss_coef = 1, bbox_loss_coef= 5, giou_loss_coef=2, mask_loss_coef = 1, \n",
    "                                                         dice_loss_coef=1,lr_backbone=1e-5, dropout=0.0,position_embedding='sine', transformer_activation='prelu',\n",
    "                                                         num_patterns =0,  random_refpoints_xy=True, hidden_dim = 256, nheads=8, num_queries=300,backbone = 'resnet50',\n",
    "                                                         num_feature_levels=4, dim_feedforward=2048, enc_layers=6, dec_layers=6, focal_alpha=0.25, num_select = 300,\n",
    "                                                         iter_update=True, query_dim =4, bbox_embed_diff_each_layer=False, pre_norm=True, aux_loss = False, masks= False)\n",
    "    #elif args.modelname.lower() == 'dab_deformable_detr':\n",
    "        #model, criterion, postprocessors = build_dab_deformable_detr(args)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return model, criterion, postprocessors\n",
    "\n",
    "def main(output_dir, rank, world_size, local_rank, frozen_weights, masks, device, seed, distributed, gpu, find_unused_params, lr_backbone, \n",
    "         lr, weight_decay, batch_size, num_workers, lr_drop, resume, pretrain_model_path, finetune_ignore, eval, start_epoch, epochs, \n",
    "         clip_max_norm, amp, use_dn, debug, save_results, save_checkpoint_interval, coco_path, fix_size,save_log=False):\n",
    "\n",
    "    \n",
    "    # setup logger\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.environ['output_dir'] = output_dir\n",
    "    logger = setup_logger(output=os.path.join(output_dir, 'info.txt'), distributed_rank=rank, color=False, name=\"DAB-DETR\")\n",
    "    logger.info(\"git:\\n  {}\\n\".format(utils.get_sha()))\n",
    "    logger.info(\"Command: \"+' '.join(sys.argv))\n",
    "    if rank == 0:\n",
    "        save_json_path = os.path.join(output_dir, \"config.json\")\n",
    "        # print(\"args:\", vars(args))\n",
    "        with open(save_json_path, 'w') as f:\n",
    "            json.dump({\n",
    "                'output_dir': output_dir,\n",
    "                'rank': rank,\n",
    "                'world_size': world_size,\n",
    "                'local_rank': local_rank,\n",
    "                'frozen_weights': frozen_weights,\n",
    "                'masks': masks,\n",
    "                \n",
    "            }, f, indent=2)\n",
    "        logger.info(\"Full config saved to {}\".format(save_json_path))\n",
    "    logger.info('world size: {}'.format(world_size))\n",
    "    logger.info('rank: {}'.format(rank))\n",
    "\n",
    "    logger.info('local_rank: {}'.format(local_rank))\n",
    "    logger.info(\"output_dir: \" + str(output_dir))\n",
    "    logger.info(\"frozen_weights: \" + str(frozen_weights))\n",
    "    logger.info(\"masks: \" + str(masks))\n",
    "\n",
    "    if frozen_weights is not None:\n",
    "        assert masks, \"Frozen training is meant for segmentation only\"\n",
    "    print('output_dir:', output_dir)\n",
    "    print('rank:', rank)\n",
    "    print('world_size:', world_size)\n",
    "    print('local_rank:', local_rank)\n",
    "    print('frozen_weights:', frozen_weights)\n",
    "    print('masks:', masks)\n",
    "\n",
    "    device = torch.device(device)\n",
    "\n",
    "    # fix the seed for reproducibility\n",
    "    seed = seed + utils.get_rank()\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    # build model\n",
    "    model, criterion, postprocessors = build_model_main('dab_detr') # modelname = 'dab_detr\n",
    "    wo_class_error = False\n",
    "    model.to(device)\n",
    "\n",
    "    model_without_ddp = model\n",
    "    if distributed:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[gpu], find_unused_parameters=find_unused_params)\n",
    "        model_without_ddp = model.module\n",
    "    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    logger.info('number of params:'+str(n_parameters))\n",
    "    logger.info(\"params:\\n\"+json.dumps({n: p.numel() for n, p in model.named_parameters() if p.requires_grad}, indent=2))\n",
    "\n",
    "\n",
    "    param_dicts = [\n",
    "        {\"params\": [p for n, p in model_without_ddp.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n",
    "        {\n",
    "            \"params\": [p for n, p in model_without_ddp.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
    "            \"lr\": lr_backbone,\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    optimizer = torch.optim.AdamW(param_dicts, lr=lr,\n",
    "                                  weight_decay=weight_decay)\n",
    "    \n",
    "\n",
    "    dataset_train = build(image_set='train', coco_path = coco_path, strong_aug= False, fix_size=fix_size)\n",
    "    dataset_val = build(image_set='val', coco_path = coco_path, strong_aug=False, fix_size=fix_size)\n",
    "\n",
    "    if distributed:\n",
    "        sampler_train = DistributedSampler(dataset_train)\n",
    "        sampler_val = DistributedSampler(dataset_val, shuffle=False)\n",
    "    else:\n",
    "        sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "        sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "\n",
    "    batch_sampler_train = torch.utils.data.BatchSampler(\n",
    "        sampler_train, batch_size, drop_last=True)\n",
    "\n",
    "    data_loader_train = DataLoader(dataset_train, batch_sampler=batch_sampler_train,\n",
    "                                   collate_fn=utils.collate_fn, num_workers=num_workers)\n",
    "    data_loader_val = DataLoader(dataset_val, batch_size, sampler=sampler_val,\n",
    "                                 drop_last=False, collate_fn=utils.collate_fn, num_workers=num_workers)\n",
    "\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, lr_drop)\n",
    "\n",
    "\n",
    "    \"\"\"if args.dataset_file == \"coco_panoptic\":\n",
    "        # We also evaluate AP during panoptic training, on original coco DS\n",
    "        coco_val = datasets.coco.build(\"val\", args)\n",
    "        base_ds = get_coco_api_from_dataset(coco_val)\n",
    "    else:\"\"\"\n",
    "    base_ds = get_coco_api_from_dataset(dataset_val)\n",
    "\n",
    "    if frozen_weights is not None:\n",
    "        checkpoint = torch.load(frozen_weights, map_location='cpu')\n",
    "        model_without_ddp.detr.load_state_dict(checkpoint['model'])\n",
    "\n",
    "    output_dir = Path(output_dir)\n",
    "    if resume:\n",
    "        if resume.startswith('https'):\n",
    "            checkpoint = torch.hub.load_state_dict_from_url(\n",
    "                resume, map_location='cpu', check_hash=True)\n",
    "        else:\n",
    "            checkpoint = torch.load(resume, map_location='cpu')\n",
    "        model_without_ddp.load_state_dict(checkpoint['model'])\n",
    "        if not eval and 'optimizer' in checkpoint and 'lr_scheduler' in checkpoint and 'epoch' in checkpoint:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "            start_epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "    if not resume and pretrain_model_path:\n",
    "        checkpoint = torch.load(pretrain_model_path, map_location='cpu')['model']\n",
    "        from collections import OrderedDict\n",
    "        _ignorekeywordlist = finetune_ignore if finetune_ignore else []\n",
    "        ignorelist = []\n",
    "\n",
    "        def check_keep(keyname, ignorekeywordlist):\n",
    "            for keyword in ignorekeywordlist:\n",
    "                if keyword in keyname:\n",
    "                    ignorelist.append(keyname)\n",
    "                    print(f'Ignoring(main L322): {keyname}')  # print the key that is being ignored\n",
    "                    return False\n",
    "            return True\n",
    "\n",
    "        logger.info(\"Ignore keys: {}\".format(json.dumps(ignorelist, indent=2)))\n",
    "        _tmp_st = OrderedDict({k:v for k, v in clean_state_dict(checkpoint).items() if check_keep(k, _ignorekeywordlist)})\n",
    "        _load_output = model_without_ddp.load_state_dict(_tmp_st, strict=False)\n",
    "        logger.info(str(_load_output))\n",
    "        \n",
    "        #import ipdb; ipdb.set_trace()\n",
    "        \n",
    "    print(\"Start eval loop\")\n",
    "    start_time = time.time()\n",
    "    if eval:\n",
    "        os.environ['EVAL_FLAG'] = 'TRUE'\n",
    "        test_stats, coco_evaluator = evaluate(model, criterion, postprocessors, data_loader_val,amp, use_dn, base_ds, device, output_dir, debug, save_results, \n",
    "                                              wo_class_error, logger=(logger if save_log else None))\n",
    "        if output_dir:\n",
    "            utils.save_on_master(coco_evaluator.coco_eval[\"bbox\"].eval, output_dir / \"eval.pth\")\n",
    "\n",
    "        log_stats = {**{f'test_{k}': v for k, v in test_stats.items()} }\n",
    "        if output_dir and utils.is_main_process():\n",
    "            with (output_dir / \"log.txt\").open(\"a\") as f:\n",
    "                f.write(json.dumps(log_stats) + \"\\n\")\n",
    "\n",
    "        return\n",
    "\n",
    "    print(\"Start training\")\n",
    "    start_time = time.time()\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        if distributed:\n",
    "            sampler_train.set_epoch(epoch)\n",
    "                \n",
    "        train_stats = train_one_epoch( use_dn,\n",
    "            model, criterion, data_loader_train, optimizer, device, epoch,\n",
    "            clip_max_norm, wo_class_error=wo_class_error, lr_scheduler=lr_scheduler,amp=False , logger=(logger if save_log else None))\n",
    "        if output_dir:\n",
    "            checkpoint_paths = [output_dir / 'checkpoint.pth']\n",
    "            # extra checkpoint before LR drop and every 100 epochs\n",
    "            if (epoch + 1) % lr_drop == 0:\n",
    "                checkpoint_paths.append(output_dir / f'checkpoint{epoch:04}_beforedrop.pth')\n",
    "            for checkpoint_path in checkpoint_paths:\n",
    "                utils.save_on_master({\n",
    "                    'model': model_without_ddp.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'lr_scheduler': lr_scheduler.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                }, checkpoint_path)\n",
    "\n",
    "        lr_scheduler.step()\n",
    "        if output_dir:\n",
    "            checkpoint_paths = [output_dir / 'checkpoint.pth']\n",
    "            # extra checkpoint before LR drop and every 100 epochs\n",
    "            if (epoch + 1) % lr_drop == 0 or (epoch + 1) % save_checkpoint_interval == 0:\n",
    "                checkpoint_paths.append(output_dir / f'checkpoint{epoch:04}.pth')\n",
    "            for checkpoint_path in checkpoint_paths:\n",
    "                utils.save_on_master({\n",
    "                    'model': model_without_ddp.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'lr_scheduler': lr_scheduler.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                }, checkpoint_path)\n",
    "        \n",
    "        test_stats, coco_evaluator = evaluate(model, criterion, postprocessors,data_loader_val,amp, use_dn, base_ds, device, output_dir, debug, save_results, \n",
    "                                              wo_class_error, logger=(logger if save_log else None))\n",
    "\n",
    "        log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
    "                     **{f'test_{k}': v for k, v in test_stats.items()},\n",
    "                     'epoch': epoch,\n",
    "                     'n_parameters': n_parameters}\n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        epoch_time_str = str(datetime.timedelta(seconds=int(epoch_time)))\n",
    "        log_stats['epoch_time'] = epoch_time_str\n",
    "\n",
    "        if output_dir and utils.is_main_process():\n",
    "            with (output_dir / \"log.txt\").open(\"a\") as f:\n",
    "                f.write(json.dumps(log_stats) + \"\\n\")\n",
    "\n",
    "            # for evaluation logs\n",
    "            if coco_evaluator is not None:\n",
    "                (output_dir / 'eval').mkdir(exist_ok=True)\n",
    "                if \"bbox\" in coco_evaluator.coco_eval:\n",
    "                    filenames = ['latest.pth']\n",
    "                    if epoch % 50 == 0:\n",
    "                        filenames.append(f'{epoch:03}.pth')\n",
    "                    for name in filenames:\n",
    "                        torch.save(coco_evaluator.coco_eval[\"bbox\"].eval,\n",
    "                                   output_dir / \"eval\" / name)\n",
    "        print(f'Epoch {epoch+1}/{epochs}')\n",
    "        print(f'Train stats: {train_stats}')\n",
    "    total_time = time.time() - start_time\n",
    "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "    print('Training time {}'.format(total_time_str))\n",
    "    print(\"Now time: {}\".format(str(datetime.datetime.now())))\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    output_dir = \"./output_model\"  # specify your output directory here\n",
    "    coco_path = \"./coco_dataset_test\"  # specify your coco dataset path here\n",
    "    if output_dir:\n",
    "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    main(output_dir= output_dir, rank=0, world_size=1, local_rank = 0, frozen_weights =None, masks = False, device ='cuda', seed = 42,\n",
    "         distributed = False, gpu=0, find_unused_params= True, lr_backbone = 1e-5, lr= 1e-4, weight_decay= 1e-4, batch_size =2, num_workers = 10, \n",
    "         lr_drop=200, resume = '', pretrain_model_path = \"./pre_train_model/checkpoint.pth\", finetune_ignore='+', eval=True, start_epoch =0, epochs=300, clip_max_norm=0.1, \n",
    "         amp = True, use_dn =True, debug=True, save_results = True, save_checkpoint_interval=100, coco_path= coco_path, fix_size=True,save_log=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
